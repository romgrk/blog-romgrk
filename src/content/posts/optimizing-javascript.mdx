---
title: 'Optimizing Javascript for fun and for profit'
description: ''
pubDate: 'Mar 20 2024'
draft: true
sidebar: { depth: 2 }
---
import Aside from '../../components/Aside.astro'
import RandomPlant from '../../components/client/RandomPlant/RandomPlant.tsx'
import { Benchmark } from './optimizing-javascript.tsx'


I often feel like javascript code in general runs much slower than it could, simply because it's not optimized properly. Here is a summary of common optimization techniques I've found useful. Note that the tradeoff for performance is often readability, so the question of when to go for performance versus readability is a question left to the reader. I'll also note that talking about optimization necessarily requires talking about benchmarking. Micro-optimizing a function for hours to have it run 100x faster is meaningless if the function only represented a fraction of the actual overall runtime to start with. If one is optimizing, the first and most important step is benchmarking. I'll cover the topic in the later points. Be also aware that micro-benchmarks are often flawed, and that may include those presented here. I've done my best to avoid those traps, but don't blindly apply any of the points presented here without benchmarking.

I have included runnable examples for all cases where it's possible. They show by default the results I got on my machine (brave 122 on archlinux) but you can run them yourself. As much as I hate to say it, Firefox has fallen a bit behind in the optimization game, and represents a very small fraction of the traffic [for now](https://foundation.mozilla.org/en/?form=donate-header), so I don't recommend using the results you'd get on Firefox as useful indicators.

## 0. Avoid work

This might sound evident, but it needs to be here because there can't be another first step to optimization: if you're trying to optimize, you should first look into avoiding work.

## 1. Avoid string comparisons

Javascript makes it easy to hides the real cost of string comparisons. If you need to compare strings in C, you'd use `strcmp(a, b)`, versus `a == b` for integers. Javascript uses `===` for both, so you don't see the `strcmp`. But it's there, and a string comparison will always require comparing each of the characters in the string; string comparison is `O(n)`. One common javascript pattern to avoid is strings-as-enums. But with the advent of typescript this should be easily avoidable, as
enums are integers by default.

<div class="code-blocks-row">

```typescript
// No
enum Position {
  TOP = 'TOP',
  BOTTOM = 'BOTTOM',
}
```

```typescript
// Yes
enum Position {
  TOP,
  BOTTOM,
}
```

</div>

Here is a comparison of the costs:

<div id="benchmark-compare" class="code-blocks-row">

```javascript
// 1. string compare
const Position = {
  TOP: 'TOP',
  BOTTOM: 'BOTTOM',
}

let _ = 0
for (let i = 0; i < 1000000; i++) {
  let current = i % 2 === 0 ?
    Position.TOP : Position.BOTTOM
  if (current === Position.TOP)
    _ += 1
}
```

```javascript
// 2. int compare
const Position = {
  TOP: 0,
  BOTTOM: 1,
}

let _ = 0
for (let i = 0; i < 1000000; i++) {
  let current = i % 2 === 0 ?
    Position.TOP : Position.BOTTOM
  if (current === Position.TOP)
    _ += 1
}
```

</div>

<Benchmark
  id="benchmark-compare"
  results={{"1. string compare":{"runTime":-1000,"amountOfRounds":828,"percent":50.35},"2. int compare":{"runTime":-1000,"amountOfRounds":2137,"percent":100}}}
  client:load
/>

As you can see, the difference can be significant. The difference isn't just due to the amount of comparisons needed, but also because integers are usually passed by value in JS engines, whereas strings are always passed as a pointer. For example, V8 represents integers as compact [Smi](https://medium.com/fhinkel/v8-internals-how-small-is-a-small-integer-e0badc18b6da) values, and JSC uses [double tagging](https://ktln2.org/2020/08/25/javascriptcore/) to the same effect, as SpiderMonkey does.

In string heavy code, this can have a huge impact. For example, I was able to [make this JSON5 javascript parser run 2x faster](https://github.com/json5/json5/pull/278) just by replacing strings by numbers.

## 2. Avoid different shapes

Javascript engines try to optimize code by assuming that objects have a specific shape, and that functions will receive objects of the same shape. This allows them to store the keys of the shape in one table common for all objects of that shape, and the values in a flat array. For example, at runtime if the following function receives two objects with the shape `{ x: number, y: number }`, the engine is going to speculate that future objects will have the same shape, and generate machine code optimized for that shape.

```javascript
function add(a, b) {
  return { x: a.x + b.x, y: a.y + b.y}
}
```

If one would instead pass an object not with the shape `{ x, y }` but with the shape `{ y, x }`, the engine would need to undo its speculation and the function would suddenly become considerably slower. I'm going to limit my explanation here because you should read  the [excellent post from mraleph](https://mrale.ph/blog/2015/01/11/whats-up-with-monomorphism.html) if you want more details, but I'm going to highlight that V8 in particular has 3 modes, for accesses that are: monomorphic (1 shape), polymorphic (2-4 shapes), and megamorphic (5+ shapes). Let's say you *really* want to stay monomorphic, because the slowdown is drastic:

<div id="benchmark-shape" class="code-blocks">

```javascript
// setup
let _ = 0
```

```javascript
// 1. monomorphic
const o1 = { a: 1, b: _, c: _, d: _, e: _ }
const o2 = { a: 1, b: _, c: _, d: _, e: _ }
const o3 = { a: 1, b: _, c: _, d: _, e: _ }
const o4 = { a: 1, b: _, c: _, d: _, e: _ }
const o5 = { a: 1, b: _, c: _, d: _, e: _ } // all shapes are equal
```

```javascript
// 2. polymorphic
const o1 = { a: 1, b: _, c: _, d: _, e: _ }
const o2 = { a: 1, b: _, c: _, d: _, e: _ }
const o3 = { a: 1, b: _, c: _, d: _, e: _ }
const o4 = { a: 1, b: _, c: _, d: _, e: _ }
const o5 = { b: _, a: 1, c: _, d: _, e: _ } // this shape is different
```

```javascript
// 3. megamorphic
const o1 = { a: 1, b: _, c: _, d: _, e: _ }
const o2 = { b: _, a: 1, c: _, d: _, e: _ }
const o3 = { b: _, c: _, a: 1, d: _, e: _ }
const o4 = { b: _, c: _, d: _, a: 1, e: _ }
const o5 = { b: _, c: _, d: _, e: _, a: 1 } // all shapes are different
```

```javascript
// test case
function add(a1, b1) {
  return a1.a + a1.b + a1.c + a1.d + a1.e +
         b1.a + b1.b + b1.c + b1.d + b1.e }

let result = 0
for (let i = 0; i < 1000000; i++) {
  result += add(o1, o2)
  result += add(o3, o4)
  result += add(o4, o5)
}
```

</div>

<Benchmark
  id="benchmark-shape"
  results={{"1. monomorphic":{"runTime":-1000,"amountOfRounds":1247,"percent":100},"2. polymorphic":{"runTime":-1003,"amountOfRounds":163,"percent":13.07},"3. megamorphic":{"runTime":-1008,"amountOfRounds":51,"percent":4.09}}}
  client:load
/>

## 3. Avoid array/object methods

I love functional programming as much as anyone else, but unless you're working in Haskell/OCaml/Rust where functional code gets compiled to efficient machine code, functional code will probably always be slower than imperative code.

```javascript
const result =
  [1.5, 3.5, 5.0]
    .map(n => Math.round(n))
    .filter(n => n % 2 === 0)
    .reduce((a, n) => a + n, 0)
```

The problem with those methods is that:

  1. They need to make a full copy of the array, and those copies later need to be freed by the garbage collector. We will explore more in details the issues of memory accesses in section 5.
  2. They loop N times for N operations, whereas a `for` loop would allow looping once.



<div id="benchmark-array-methods" class="code-blocks">

```javascript
// setup:
const numbers = Array.from({ length: 10_000 }).map(() => Math.random())
```

```javascript
// 1. functional
const result =
  numbers
    .map(n => Math.round(n * 10))
    .filter(n => n % 2 === 0)
    .reduce((a, n) => a + n, 0)
```

```javascript
// 2. imperative
let result = 0
for (let i = 0; i < numbers.length; i++) {
  let n = Math.round(numbers[i] * 10)
  if (n % 2 !== 0) continue
  result = result + n
}
```

</div>

<Benchmark
  id="benchmark-array-methods"
  results={{"1. functional":{"runTime":-1002,"amountOfRounds":303,"percent":36.51},"2. imperative":{"runTime":-1000,"amountOfRounds":830,"percent":100}}}
  client:load
/>

<br />

Object methods such as `Object.values()`, `Object.keys()` and `Object.entries()` suffer from similar problems, as they also allocate more data, and memory accesses are the root of all performance issues. No really I swear, I'll show you in section 5.

## 4. Avoid indirection

Another place to look for optimization gains is any source of indirection, of which I can see 3 main sources:

```javascript
const point = { x: 10, y: 20 }

// 1.
// Proxy objects are harder to optimize because their get/set function might
// be running custom logic, so engines can't make their usual assumptions.
const proxy = new Proxy(point, { get: (t, k) => { return t[k] } })
// Some engines can make proxy costs disappear, but those optimizations are
// expensive to make and can break easily.
const x = proxy.x

// 2.
// Usually ignored, but accessing an object via `.` or `[]` is also an
// indirection. In easy cases, the engine may very well be able to optimize the
// cost away:
const x = point.x
// But each additional access multiplies the cost, and makes it harder for the
// engine to make assumptions about the state of `point`:
const x = this.state.circle.center.point.x

// 3.
// And finally, function calls can also have a cost. Engine are generally good
// at inlining these:
function getX(p) { return p.x }
const x = getX(p)
// But it's not garanteed that they can. In particular if the function call
// isn't from a static function but comes from e.g. an argument:
function Component({ point, getX }) {
  return getX(point)
}
```

The proxy benchmark is particularly brutal on V8 at the moment. Last time I checked, proxy objects were always falling back from the JIT to the interpreter, seeing from those results it might still be the case.

<div id="benchmark-proxy" class="code-blocks">


```javascript
// 1. proxy access
const point = new Proxy({ x: 10, y: 20 }, { get: (t, k) => t[k] })

for (let _ = 0, i = 0; i < 100_000; i++) { _ += point.x }
```

```javascript
// 2. direct access
const point = { x: 10, y: 20 }
const x = point.x

for (let _ = 0, i = 0; i < 100_000; i++) { _ += x }
```

</div>

<Benchmark
  id="benchmark-proxy"
  results={{"1. proxy access":{"runTime":-1000,"amountOfRounds":659,"percent":2.8},"2. direct access":{"runTime":-1000,"amountOfRounds":23544,"percent":100}}}
  client:load
/>

<div id="benchmark-object" class="code-blocks">

I also wanted to showcase accessing a deeply nested object vs direct access, but engines are very good at [optimizing away object accesses via escape analysis](https://youtu.be/KiWEWLwQ3oI?t=1055) when there is a hot loop and a constant object. I had to insert a bit of indirection to prevent it.

```javascript
// 1. nested access
const a = { state: { center: { point: { x: 10, y: 20 } } } }
const b = { state: { center: { point: { x: 10, y: 20 } } } }
const get = (i) => i % 2 ? a : b

let result = 0
for (let i = 0; i < 100_000; i++) {
  result = result + get(i).state.center.point.x }
```

```javascript
// 2. direct access
const a = { x: 10, y: 20 }.x
const b = { x: 10, y: 20 }.x
const get = (i) => i % 2 ? a : b

let result = 0
for (let i = 0; i < 100_000; i++) {
  result = result + get(i) }
```

</div>

<Benchmark
  id="benchmark-object"
  results={{"1. nested access":{"runTime":-1000,"amountOfRounds":9871,"percent":42.08},"2. direct access":{"runTime":-1000,"amountOfRounds":23460,"percent":100}}}
  client:load
/>


## 5. Avoid cache misses

This point requires a bit of low-level knowledge, but has implications even in javascript, so I'll explain. From the CPU point of view, retrieving memory from RAM is slow. To speed things up, it uses mainly two optimizations.

### 5.1 Prefetching

The first one is prefetching: it fetches more memory ahead of time, in the hope that it's the memory you'll be interested in. It always guesse that if you request one memory address, you'll be interested in the memory region that comes right after that. So **accessing data sequentially** is they key. In the following example, we can observe the impact of accessing memory in random order.

<div id="benchmark-prefetch" class="code-blocks">

<div class="hidden">

```javascript
// setup:
function shuffle(array) {
  let currentIndex = array.length,  randomIndex;

  while (currentIndex > 0) {
    randomIndex = Math.floor(Math.random() * currentIndex);
    currentIndex--;
    [array[currentIndex], array[randomIndex]] = [
      array[randomIndex], array[currentIndex]];
  }

  return array;
}
```

</div>

```javascript
// setup:
const K = 1024
const length = 1 * K * K

// Theses points are created one after the other, so they are allocated
// sequentially in memory.
const points = new Array(length)
for (let i = 0; i < points.length; i++) {
  points[i] = { x: 42, y: 0 }
}

// This array contains the *same data* as above, but shuffled randomly.
const shuffledPoints = shuffle(points.slice())
```

```javascript
// 1. sequential
let _ = 0
for (let i = 0; i < points.length; i++) { _ += points[i].x }
```

```javascript
// 2. random
let _ = 0
for (let i = 0; i < shuffledPoints.length; i++) { _ += shuffledPoints[i].x }
```

</div>

<Benchmark
  id="benchmark-prefetch"
  results={{"2. random":{"runTime":-1000,"amountOfRounds":226,"percent":26.22},"1. sequential":{"runTime":-1000,"amountOfRounds":862,"percent":100}}}
  client:load
/>

#### What the eff should I do about this?

This aspect is probably the hardest to put in practice, because javascript doesn't have a way of placing objects in memory. You cannot assume that objects created sequentially will stay at the same location because the garbage collector might move them around. There is one exception to that, and it's arrays of numbers, preferably `TypedArray` instances:

```javascript
// from this
const points = [{ x: 0, y: 5 }, { x: 0, y: 10 }]

// to this
const points = new Int64Array([0, 5, 0, 10])
```

For a more detailed example, [see this link](https://mrale.ph/blog/2018/02/03/maybe-you-dont-need-rust-to-speed-up-your-js.html#optimizing-parsing---reducing-gc-pressure).

### 5.2 Caching in L1/2/3

The second optimization CPUs use is the L1/L2/L3 caches: those are like faster RAMs, but they are also more expensive, so they are much smaller. The contain RAM data, but act as an LRU cache. Data comes in while it's "hot" (being worked on), and written back to the main RAM when new working data needs the space. So the key here is **use as few data as possible to keep you working dataset in the fast caches**. In the following example, we can observe what are the effects of busting each of the successive caches.

<div id="benchmark-caches" class="code-blocks">

```javascript
// setup:
const KB = 1024
const MB = 1024 * KB

// These are approximate sizes to fit in those caches. If you don't get the
// same results on your machine, it might be because your sizes differ.
const L1  = 256 * KB
const L2  =   5 * MB
const L3  =  18 * MB
const RAM =  32 * MB

// We'll be accessing the same buffer for all test cases, but we'll
// only be accessing the first 0 to `L1` entries in the first case,
// 0 to `L2` in the second, etc.
const buffer = new Int8Array(RAM)
buffer.fill(42)

const random = (max) => Math.floor(Math.random() * max)
```

```javascript
// 1. L1
let r = 0; for (let i = 0; i < 100000; i++) { r += buffer[random(L1)] }
```

```javascript
// 2. L2
let r = 0; for (let i = 0; i < 100000; i++) { r += buffer[random(L2)] }
```

```javascript
// 3. L3
let r = 0; for (let i = 0; i < 100000; i++) { r += buffer[random(L3)] }
```

```javascript
// 4. RAM
let r = 0; for (let i = 0; i < 100000; i++) { r += buffer[random(RAM)] }
```

</div>

<Benchmark
  id="benchmark-caches"
  results={{"1. L1":{"runTime":-1000,"amountOfRounds":1800,"percent":100},"2. L2":{"runTime":-1000,"amountOfRounds":1405,"percent":78.06},"3. L3":{"runTime":-1000,"amountOfRounds":1186,"percent":65.89},"4. RAM":{"runTime":-1002,"amountOfRounds":528,"percent":29.33}}}
  client:load
/>

#### What the eff should I do about this?

**Ruthlessly eliminate every single data or memory allocations** that can be eliminated. The smaller your dataset is, the faster your program will run. Memory I/O is the bottleneck for 95% of programs. Another good strategy can be to split your work into chunks, and ensure you work on a small dataset at a time.

For more details on CPU and memory, [see this link](https://www.akkadia.org/drepper/cpumemory.pdf).

<Aside title="About immutable data structures">
  Immutability is great for clarity and correctness, but in the context of performance, updating an immutable data structure means making a copy of the container, and that's more memory I/O that will flush your caches. You should avoid immutable data structures when possible.
</Aside>

<Aside title="About the { ...spread } operator ">
  It's very convenient, but every time you use it you create a new object in memory. More memory I/O, slower caches!
</Aside>

<br />
<RandomPlant />
<br />

## 6. Avoid large objects

As explained in section 2, engines use shapes to optimize objects. However, when the shape grows too large, the engine has no choice but to use a regular hashmap (like a `Map` object). And as we saw in section 5, cache misses decrease performance significantly. Hashmaps are prone to this because their data is usually randomly & evenly distributed over the memory region they occupy. Let's see how it behaves with this map of some users indexed by their ID.

<div id="benchmark-large-object" class="code-blocks">

```javascript
// setup:
const USERS_LENGTH = 1_000
```

```javascript
// setup:
const byId = {}
Array.from({ length: USERS_LENGTH }).forEach((_, id) => {
  byId[id] = { id, name: 'John'}
})
let _ = 0
```

```javascript
// 1. [] access
Object.keys(byId).forEach(id => { _ += byId[id].id })
```

```javascript
// 2. direct access
Object.values(byId).forEach(user => { _ += user.id })
```

</div>

<Benchmark
  id="benchmark-large-object"
  results={{"1. [] access":{"runTime":-1000,"amountOfRounds":49173,"percent":43.18},"2. direct access":{"runTime":-1000,"amountOfRounds":113887,"percent":100}}}
  client:load
/>

And we can also observe how the performance keeps degrading as the object size grows:

<div id="benchmark-large-object-larger" class="code-blocks">

```javascript
// setup:
const USERS_LENGTH = 100_000
```

<div class="hidden">

```javascript
// setup:
const byId = {}
Array.from({ length: USERS_LENGTH }).forEach((_, id) => {
  byId[id] = { id, name: 'John'}
})
let _ = 0
```

```javascript
// 1. [] access
Object.keys(byId).forEach(id => { _ += byId[id].id })
```

```javascript
// 2. direct access
Object.values(byId).forEach(user => { _ += user.id })
```

</div>

</div>

<Benchmark
  id="benchmark-large-object-larger"
  results={{"1. [] access":{"runTime":-1005,"amountOfRounds":229,"percent":20.67},"2. direct access":{"runTime":-1000,"amountOfRounds":1108,"percent":100}}}
  client:load
/>

#### What the eff should I do about this?

As demonstrated above, avoid having to frequently index into large objects. Prefer turning the object into an array beforehand. Organizing your data to have the ID on the model can help, as you can use `Object.values()` and not have to refer to the key map to get the ID.

## 7. Use eval

Some javascript patterns are hard to optimize for engines, and by using `eval()` or its derivatives you can make those patterns disappear. In this example, we can observe how using `eval()` avoids the cost of creating an object with a dynamic object key:


<div id="benchmark-eval" class="code-blocks">

```javascript
// setup:
const key = 'requestId'
const values = Array.from({ length: 100_000 }).fill(42)
```

```javascript
// 1. without eval
function createMessages(key, values) {
  const messages = []
  for (let i = 0; i < values.length; i++) {
    messages.push({ [key]: values[i] })
  }
  return messages
}

createMessages(key, values)
```

```javascript
// 2. with eval
function createMessages(key, values) {
  const messages = []
  const createMessage = new Function('value',
    `return { ${JSON.stringify(key)}: value }`
  )
  for (let i = 0; i < values.length; i++) {
    messages.push(createMessage(values[i]))
  }
  return messages
}

createMessages(key, values)
```

</div>

<Benchmark
  id="benchmark-eval"
  results={{"1. without eval":{"runTime":-1005,"amountOfRounds":233,"percent":53.2},"2. with eval":{"runTime":-1001,"amountOfRounds":438,"percent":100}}}
  client:load
/>

Obviously the usual warnings about `eval()` apply: don't trust user input, sanitize anything that gets passed into the `eval()`'d code, and don't create any XSS possibility. Also note that some environments don't allow access to `eval()`, such as browser pages with a CSP.


## 8. Use strings carefully

We've already seen above how strings are more expensive than they appear. Well I have kind of a good news/bad news situation here, which I'll announce in the only logical order (bad first, good second). Strings are more complex than they appear. But they can also be quite efficient in some situations.

String operations are a core part of javascript due to its context. To optimize string-heavy code, engines had to be creative. And by that I mean, they had to represent the `String` object with multiple string representation in C++, depending on the use case. There are two general cases you should worry about, because they hold true for V8 (the most common engine by far), and generally also in other engines.

First, strings concatenated with `+` don't create a copy of the two input strings. It creates a pointer to each substring. If it was in typescript, it would be something like this:

```typescript
class String {
  abstract value(): char[] {}
}

class BytesString {
  constructor(bytes: char[]) {
    this.bytes = bytes
  }
  value() {
    return this.bytes
  }
}

class ConcatenatedString {
  constructor(left: String, right: String) {
    this.left = left
    this.right = right
  }
  value() {
    return [...this.left.value(), ...this.right.value()]
  }
}

function concat(left, right) {
  return new ConcatenatedString(left, right)
}

const first = new BytesString(['H', 'e', 'l', 'l', 'o', ' '])
const second = new BytesString(['w', 'o', 'r', 'l', 'd'])

// See ma, no array copies!
const message = concat(first, second)
```

Second, string slices also don't need to create copies: they can simply point to a range in another string. To continue with the example above:

```typescript
class SlicedString {
  constructor(source: String, start: number, end: number) {
    this.source = source
    this.start = start
    this.end = end
  }
  value() {
    return this.source.value().slice(this.start, this.end)
  }
}

function substring(source, start, end) {
  return new SlicedString(source, start, end)
}

// This represents "He", but it still contains no array copies.
// It's a SlicedString to a ConcatenatedString to two BytesString
const firstTwoLetters = substring(message, 0, 2)
```

But here's the issue: once you need to start mutating those bytes, that's the moment you start paying copy costs. Let's say we go back to our `String` class and try to add a `.trimEnd` method:

```typescript
class String {
  abstract value(): char[] {}

  trimEnd() {
    // `.value()` here might be calling
    // our Sliced->Concatenated->2*Bytes string!
    const bytes = this.value()

    const result = bytes.slice()
    while (result[result.length - 1] === ' ')
      result.pop()
    return new BytesString(result)
  }
}
```

So let's jump to an example where we compare using operations that use mutation versus only using concatenation:

<div id="benchmark-strings" class="code-blocks">

```javascript
// setup:
const classNames = ['primary', 'selected', 'active', 'medium']
```

```javascript
// 1. mutation
const result =
  classNames
    .map(c => `button--${c}`)
    .join(' ')
```

```javascript
// 2. concatenation
const result =
  classNames
    .map(c => 'button--' + c)
    .reduce((acc, c) => acc + ' ' + c, '')
```

</div>

<Benchmark
  id="benchmark-strings"
  iterations={5000}
  results={{"1. mutation":{"runTime":-1000,"amountOfRounds":1808,"percent":37.43},"2. concatenation":{"runTime":-1000,"amountOfRounds":4830,"percent":100}}}
  client:load
/>

#### What the eff should I do about this?

There is a lot of complexity to strings across all the engines. I've skimmed very quickly, but there are often minimum lengths for each of those string representations, for example a concatenated string might not be used for very small strings. Or sometimes there are limits, for example avoiding pointing to a substring of a substring. In general, try to **avoid mutation for as long as possible**. This includes methods such as `.trim()`, `.replace()`, etc. In some engines, string templates can also be slower than `+`. In V8 at the moment, it's the case. This is true now, but might not be in the future. The most important thing is: __benchmark__.

Final note, but one of the implications of the `SlicedString` above is that if a small substring to a very large string is alive in memory, it might prevent the garbage collector from collecting the huge string! This is the case in V8. If you're processing large texts and extracting small strings from it, you might be leaking large amounts of memory.

```javascript
const large = Array.from({ length: 10_000 }).map(() => 'string').join('').trim()
// will keep alive `large`!
const small = large.slice(10, 15)
```

The solution here is to use mutation methods to our advantage. If we use one of them on `small`, it will force a copy, and the old pointer to `large` will be lost:

```javascript
// replace a token that doesn't exist
const small = small.replace('#'.repeat(small.length + 1), '')
```


For more details, see [string.h on V8](https://github.com/v8/v8/blob/main/src/objects/string.h) or [JSString.h on JavaScriptCore](https://github.com/WebKit/WebKit/blob/main/Source/JavaScriptCore/runtime/JSString.h).


## 9. Benchmarking

I'll list the common things to think about when benchmarking:

### 9.1 Avoid micro-benchmarks

Run your code in production mode and base your optimizations on those observations. JS engines are very complex, and will often behave differently in micro-benchmarks than in real-world scenarios. For example, take this micro-benchmark:

```javascript
const a = { type: 'div', count: 5, }
const b = { type: 'span', count: 10 }

function typeEquals(a, b) {
  return a.type === b.type
}

for (let i = 0; i < 100_000; i++) {
  typeEquals(a, b)
}
```

If you've payed attention sooner, you will realize that the engine will specialize the function for the shape `{ type: string, count: number }`. But does that hold true in your real-world use-case? Is `props` always of that shape, or do you also need to support any kind of shape? If you receive many shapes in production, this function will behave differently then.

### 9.2 Doubt your results

If you've just optimized a function and it now runs 100x faster, doubt it. Try to disprove your results, try them in production mode, throw stuff at it.

### 9.3 Pick your target

Different engines will optimize certain patterns better or worst than others. You should benchmark for the engine(s) that are relevant to you, and prioritize which one is more important. [Here's a real-world example](https://github.com/babel/babel/pull/16357) in Babel where improving V8 means decreasing JSC's performance.

## 10. Profiling

Various remarks about devtools and profilers.

### 10.1

If you're profiling in the browser, make sure you use a clean and empty browser profile. I even use a separate browser for this. If you're profiling with your own profile and have browser extensions enabled, they can mess up the measurements. React devtools in particular will substantially affect results, rendering code may appear slower than it appears ~in the mirror~ to your users.

### 10.2

Browser profiling tools are sample-based profilers, which take a sample of your stack at regular intervals. This had a big disadvantage: very small but frequent functions might be called between those samples, and might be very much underreported in the stack charts you'll get. Use Firefox devtools with a custom sample interval or Chrome devtools with CPU throttling to avoid this issue.

### 10.3

Keep up with devtools, each of them have particular features than can be useful, for example: https://github.com/iamakulov/devtools-perf-features

## Final notes

Hope you learned some useful tricks, if you have any comments or corrections, email link in the footer.

<br />
<br />
<RandomPlant />
<br />
<br />


{/*

<div id="benchmark-alloc" class="code-blocks">


```javascript
// 1. Object.keys()
function cloneWithoutProps(object, excluded) {
  const result = {}
  const keys = Object.keys(object)
  for (let i = 0; i < keys.length; i++) {
    const key = keys[i]
    if (excluded.indexOf(key) >= 0) continue
    result[key] = object[key]
  }
  return result;
}
```

```javascript
// 2. No allocations
function cloneWithoutProps(object, excluded) {
  const result = {}
  for (const key in object) {
    if (object.hasOwnProperty(key)) {
      if (excluded.indexOf(key) >= 0) continue
      result[key] = object[key]
    }
  }
  return result;
}
```

```javascript
// test case
const object = { a: 1, b: 2, c: 3, d: 4, e: 5 }
const excluded = ['d', 'e']
for (let i = 0; i < 1000000; i++) {
  const result = cloneWithoutProps(object, excluded)
}
```

</div>

<Benchmark
  id="benchmark-alloc"
  results={{"1. Object.keys()":{"runTime":-1023,"amountOfRounds":15,"percent":75},"2. No allocations":{"runTime":-1035,"amountOfRounds":20,"percent":100}}}
  client:load
/>

For the same logic, we were able to save 25% of the cost simply by avoiding `Object.keys()`. This function by the way is what [babel desugars object destructuring into](https://github.com/babel/babel/pull/16357)! So if you destructured an object with 100 keys, not only were you making a copy of that object, but you were also allocating an array of a 100 keys. Hopefully it should be resolved by the time you read this.

I'll note that avoiding allocations by using imperative code is the place where the performance vs readability tradeoff is the most apparent, so using imperative code to replace something as readable as this example below, requires consideration.

```javascript
const result =
  Object.values(point)
    .map(n => n * 2)
    .filter(n => n % 2 === 0)
    .reduce((a, n) => a + n, 0)

// I'm not even going to translate it to imperative code, it would ruin
// the beauty.
```

*/}
