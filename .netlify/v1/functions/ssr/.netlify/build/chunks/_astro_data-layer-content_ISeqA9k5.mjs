const _astro_dataLayerContent = [["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.9.1","content-config-digest","07b459027aa8e1c9","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://romgrk.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"server\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":false,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"experimentalDefaultStyles\":true},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark-dimmed\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[[null,{\"theme\":\"github-dark-dimmed\",\"tokensMap\":{}}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"csp\":false},\"legacy\":{\"collections\":false},\"session\":{\"driver\":\"fs-lite\",\"options\":{\"base\":\"/home/romgrk/src/blog-romgrk/node_modules/.astro/sessions\"}}}","posts",["Map",11,12,24,25,37,38,47,48,57,58,69,70,80,81,89,90,99,100,110,111,121,122,130,131],"back-off-vercel",{id:11,data:13,body:20,filePath:21,digest:22,legacyId:23,deferredRender:19},{title:14,description:15,pubDate:16,sidebar:17,draft:19},"There is a problem with Vercel","romgrk on capitalism","Jun 13 2024",{display:18},false,true,"import RandomPlant from '../../components/client/RandomPlant/RandomPlant.tsx'\nimport Aside from '../../components/Aside.astro'\n\nI've been holding off on writing this post for a while, but I think today is a good day. As most people know, React was started by facebook/meta. Over the last couple years however, a company has been progressively buying (hiring) core react contributors, to the point that in the last year, a large part of the changes that went into react were authored by Vercel-employed contributors. Which part you may ask? I did a quick run of `git-quick-stats` for all the commits over the last year on React repository, and here is the results for all contributors over 2%:\n\n```\n4375 Noah Lemen (2%)            [META]\n7122 Jack Pope (3%)             [META]\n14077 Jan Kassens (6%)          [META]\n15291 Sebastian Silbermann (7%) [VERCEL]\n24125 Rick Hanlon (10%)         [META]\n25050 Ruslan Lesiutin (11%)     [META]\n25240 Andrew Clark (11%)        [VERCEL]\n41302 Josh Story (18%)          [VERCEL]\n58361 Sebastian Markbåge (25%)  [VERCEL]\n```\n\n**61% of lines changed were authored by Vercel employees!**\n\nNow what's the difference, you may ask, between a for-profit corporation like Meta and a for-profit corporation like Vercel? Aren't they the same thing? Well not really. There's a fundamental difference. Meta developped Re\n\nhttps://www.gomomento.com/blog/rip-redis-how-garantia-data-pulled-off-the-biggest-heist-in-open-source-history\n[why isn't vite listed on react.dev?](https://www.reddit.com/r/reactjs/comments/1d0l6fh/comment/l5o0qhu/)\n\nhttps://www.reddit.com/r/reactjs/comments/1deoxkp/react_19_broke_suspense_parallel_rendering_and/\n\nhttps://www.reddit.com/r/reactjs/comments/1dep9lm/server_components_may_not_be_as_generalizable_as/\n\n1. overengineered bad solutions\n2. greed\n\n<RandomPlant className='mt-8 mb-16' />","src/content/posts/back-off-vercel.mdx","4016f7680da8b430","back-off-vercel.mdx","color-bits",{id:24,data:26,body:33,filePath:34,digest:35,legacyId:36,deferredRender:19},{title:27,description:28,pubDate:29,sidebar:30,rating:32},"The fastest JS color library","","Sep 3 2024",{display:19,depth:31},2,8,"import Aside from '../../components/Aside.astro'\nimport RandomPlant from '../../components/client/RandomPlant/RandomPlant.tsx'\nimport Benchmark from '../../components/client/Benchmark.tsx'\nimport BytePattern from '../../components/client/BytePattern.tsx'\nimport CodeRunner from '../../components/client/CodeRunner.tsx'\n\nRecently I spent some time optimizing the performance of our color manipulation code [at work](https://mui.com/blog/material-ui-v6-is-out/), and I'm fairly gruntled with the results so I'm releasing the library—[color-bits](https://github.com/romgrk/color-bits)—as a standalone as I think it can be useful for other use-cases. I wanted to dive here a bit more in depth into what makes it fast, as it ties well into my last post on performance optimization, and I'll also be presenting a few benchmarks to illustrate the points.\n\n\n## 1. Representation\n\nIn javascript the usual way to store RGBA colors would be something like this:\n\n```javascript\nconst color = { red: 0, green: 0, blue: 0, alpha: 1.0 }\n```\n\nThis representation is idiomatic and readable, but it also implies allocating a new heap object for each color. Knowing that RGBA colors go from `#00000000` to `#ffffffff` (or in others words, the numbers from `0x0000000` to `0xffffffff`) and that it's 32 bits of data, we only really just need a single number value to encode those few bits.\n\nLucky us, that's precisely the amount we have available through javascript bitwise operators:\n\n```javascript\nconst color =\n  (red   << 24) |\n  (green << 16) |\n  (blue  <<  8) |\n  (alpha <<  0)\n```\n\nSo let's jump quickly in a benchmark to compare how the two approaches fare. In the following example, we fill a 100-length array with some shades of blue. Why a 100-length array and not just one big array with all the colors? Because it reflects what happens in a production context. You have colors going through the system, then those colors are discarded. Those discarded values must then be processed by the garbage collector, which explains...\n\n<div id=\"benchmark-compare\" class=\"code-blocks-row\">\n\n```javascript\n// 1. object\nconst newColor = (blue) => ({\n  red: 0,\n  green: 0,\n  blue: blue,\n  alpha: 1.0,\n})\n\nconst colors = new Array(100)\nfor (let i = 0; i < 100_000; i++) {\n  const index = i % 100\n  colors[index] = newColor(index)\n}\n```\n\n```javascript\n// 2. number\nconst newColor = (blue) =>\n  (   0 << 24) |\n  (   0 << 16) |\n  (blue <<  8) |\n  ( 255 <<  0)\n\n\nconst colors = new Array(100)\nfor (let i = 0; i < 100_000; i++) {\n  const index = i % 100\n  colors[index] = newColor(index)\n}\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-compare\"\n  results={{\"1. object\":{\"runTime\":-1000,\"amountOfRounds\":4031,\"percent\":35.82},\"2. number\":{\"runTime\":-1000,\"amountOfRounds\":11255,\"percent\":100}}}\n  client:load\n/>\n\n\nIt's interesting to take a look at the stack traces for that benchmark as well. On the left is the `object` representation case, and on the right the `number` one. As we can see, the `object` one is constantly creating pressure for the garbage collector—each of those ticks is a \"Minor GC\" entry. **Creating objects is expensive!**\n\n![Stack traces for the object vs number benchmark](/color-bits-stack.png)\n*Stack traces for the object vs number benchmark*\n\n\nSo let's just use `number` values and...\n\n### All is good, right?\n\nAs I was happily piping those numbers through the test code, I realized to my big disappointment that javascript bitwise operators operate on 32-bits...signed.\n\n<CodeRunner\n  autoRun\n  consoleLines={1}\n  codeLines={2}\n  code={`\n    const red = 0xff\n    expect(red << 24).toEqual(0xff000000)\n  `}\n  context={`\n    function expect(value) {\n      return {\n        toEqual: function(expected) {\n          if (value !== expected) {\n            console.log('ERROR: Expected ' + expected + ' but got ' + value)\n          }\n        }\n      }\n    }\n  `}\n  client:load\n/>\n\nSo instead of `0xff << 24` being equal to `0xff000000`, it equals `-0x1000000`. Our precious bits are all mangled by the bitwise operators! As I was running through my mind for solutions, my first thought was obviously to cast back the results from `int32` to `uint32`. After all, it's not more complicated than this:\n\n```javascript\nfunction cast(int32Value) {\n  return int32Value >>> 0\n}\n```\n\nAs a reminder, all bitwise operators turn their values into `int32` numbers, *except* for `>>>` which is the \"unsigned right shift\" operation and the only one that operates on `uint32` values.\n\nSo I was quite unhappy about adding more instructions, but hey, at least it's still all bitwise operations on numbers, which are pretty cheap. Right?\n\nWell the more I benchmarked, the more I found that solution to be unsatisfying. The problem isn't merely the additional operation, it's also that some particular engine applies a particular performance optimization that speeds up numbers up to... the 32-bits signed range. Namely, V8, the one that runs on 70% of browsing devices, and that powers NodeJS. So as soon as a color would overflow the `int32` range (in other words, any color greater than or equal to `0x80000000`), the whole codebase would slow down by a substantial factor! Red-ish colors were more expensive than blue-ish or green-ish colors, and it didn't sit well with me. So let's dive into what's happening.\n\n#### Number representation in V8\n\n<Aside title=\"About CPU architecture\">\n  I will be assuming a 64-bits architecture because that's most devices that run a JS engine today, but you can get the full picture in https://v8.dev/blog/pointer-compression.\n</Aside>\n\nIn a common device, passing values up to 64-bits is cheap because you can pass the value through CPU registers directly—their length is 64-bits. Values or objects above that size must be stored in memory, and passed as a (64-bits) pointer to that location. And pointers normally should make up all of JS values, because the garbage collector also needs to scan the heap for pointers to figure out which objects are not referenced anymore and can be freed. If there were numbers mixed with pointers, the GC wouldn't be able to know if the value is a pointer or a number.\n\nBut 64-bits of pointer location, that's <abbr title=\"18,446,744,073,709,552,000\">a lot</abbr> of bytes, and maybe we don't really need all those addresses. So V8 says we'll use one of those bits to tag if the value is a pointer or a number, and if it's a number, we'll store 32-bits of data in there, and we'll call that a \"Smi\" because it's a beautiful name.\n\n```text\n                                            tag -->|\n            |----- 32 bits -----|----- 32 bits -----|\nPointer:    |________________address_______________1|\nSmi:        |____int32_value____|0000000000000000000|\n```\n\nSo there you go, that's why you don't want to go over the `int32` range in V8. If your number overflows the `int32` range, it becomes a pointer to a number value elsewhere in the heap. We can run a benchmark to make sure that's the case. This is an example where we just add up the same value a bunch of times, but it's either inside the Smi range, or outside. I'm going to include the numbers instead of a live one because I want to show the performance across engines, but the code is available [here](https://github.com/romgrk/js-benchmark/blob/0a3245640a0a76d5700a54dc41ee8d3dab34da4b/benchmarks/integers.js) if you want to run it yourself:\n\n```text\n> node ./index.js ./benchmarks/integers.js\n\n### node: v22.7.0 ###\nadding 2^31 - 1: ##############################  100.00% (1,685,399 ops)\nadding 2^31 + 1: #####################.........   72.36% (1,219,483 ops)\n\n\n### bun: 1.1.26 ###\nadding 2^31 - 1: ##############################  100.00% (2,184,201 ops)\nadding 2^31 + 1: #####.........................   18.77%   (409,935 ops)\n\n\n### gjs: 1.80.2 ###\nadding 2^31 - 1: ##############################  100.00% (1,169,879 ops)\nadding 2^31 + 1: ##################............   62.74%   (733,938 ops)\n```\n\nThis should be taken with a grain of salt because it's a microbenchmark, but it still makes it painfully clear that overflowing the `int32` range even by a tiny `+1` makes a significant different across all engines for math operations. I don't know enough about the other engines (`bun` is JSC-based, and `gjs` is SpiderMonkey-based by the way) to explain why it has so much of an influence, but while we're here we might as well look into...\n\n#### Number representation in JSC/SpiderMonkey\n\nBoth engines use variations of a technique called NaN-boxing. Double values, also known as `float64` or `f64` in more pragmatic languages, or IEEE-754 in technical terms, is what the EcmaScript spec defines as `number`, and they are encoded in 64-bits as such:\n\n```text\n       1 bit sign\n      |-|-- 11 bits ---|------------ 52 bits ---------------|\n      |±|___exponent___|_____________mantissa_______________|\n```\n\n<Aside title=\"IEEE-755 in details\">\n\nThe formula to turn those bits into a number is the following one:\n```javascript\n  Math.pow(-1, sign) * Math.pow(2, 1023 - exponent) * (1 + mantissa)\n```\n\n<br />\n\nAnd for an example, this is how `1` is represented in f64 encoding:\n\n```text\n      0_____01111111111______________000000000000...00000000000000\n (-1)^0  *  2^(1023 - 1023)  *  (1 + 0)\n     1   *  1                *   1\n   = 1\n```\n<br />\n\n[Interactive visualization](https://bartaz.github.io/ieee754-visualization/) | [Wikipedia](https://en.wikipedia.org/wiki/Double-precision_floating-point_format)\n\n</Aside>\n\nThe neat trick is, `float64` encoding defines `NaN` as any 64-bits pattern where the exponent bits are all set to `1` and the mantissa is non-zero. So for example, both `0b0_11111111111_1000...` and `0b0_11111111111_0100...` represent `NaN`, and so forth. Instead of wasting those values, the engine can pick a single way to represent `NaN`, and use the rest of those bits to put pointers, integers, etc.\n\nAs a sidenote, this is also the reason why `Number.MAX_SAFE_INTEGER` equals `2^53 - 1`, that's as much precision as `float64` allows. The exponent uses the 11 other bits that make it to a 64 bits total.\n\n\nBut I'm digressing, so let's go back to the issue.\n\n### So how do we use int32?\n\nAs it became apparent that with the language and all the engines conspiring against me, I wasn't going to get any `uint32` anytime soon. That's when I realized the very simple solution that had been there all along: **do nothing**.\n\nLet me recap how negative numbers and two's complement work. Taking a single byte (a `uint8` value) for simplicity, here is how each bit relates to its value:\n\n```text\nIndex: |   8  |  7  |  6  |  5  |  4  |  3  |  2  |  1  | \n       |------|-----|-----|-----|-----|-----|-----|-----|\nValue: | -2^7 | 2^6 | 2^5 | 2^4 | 2^3 | 2^2 | 2^1 | 2^0 |\n```\n\nSo for the following bit patterns, the corresponding values are:\n\n<BytePattern initialValue='00000101' client:load />\n\nAnd for negative values, if we set the high bit:\n\n<BytePattern initialValue='10000000' client:load />\n\nAnd a last one, for `-1`:\n\n<BytePattern initialValue='11111111' client:load />\n\nSo why pick a scheme like two's complement instead something more simple? After all, `float64` encoding uses the first bit to signal `+/-`, isn't that more simple? The last bit pattern here might have given you a clue. The idea behind two's complement is that for some operations like say addition, we don't need separate machine instructions for signed & unsigned values. The bits stay the same, but their interpretation changes. So for example, if you subtract `1` from `0b00000000`, the bits would be `0b11111111` in both signed mode (where the value is `255`) and unsigned mode (where the value is `-1`). Using this convention simplifies CPUs, which in turns allows them to be leaner & faster. This means that when you compile in a native language a data type like `u32` or `i32`, the type disappear after compilation and the CPU only sees untyped bits. Some operations still require special handling for signed/unsigned numbers, but in those cases the instruction itself will be typed, not the data. For example, `x86` has the `MUL` instruction for unsigned multiplication and `IMUL` for signed multiplication.\n\nI hope this all explains my previous epiphany about **doing nothing**. Our bits were never mangled by bitwise operators, the bits were where they were supposed to be all along. If you `0xff << 24`, the underlying bit pattern will still be `0b111111110000....0000` regardless if the number is interpreted as signed or unsigned.\n\nI think it's a very normal thing to reach for `uint32`, it's the logical representation for 4 packed bytes. But in the limited context of javascript, going for what is available instead is the better option. I've seen other libraries as well do the same mistake, for example I was reading [react-spring's color handling code](https://github.com/pmndrs/react-spring/blob/fd65b605b85c3a24143c4ce9dd322fdfca9c66be/packages/shared/src/normalizeColor.ts#L48-L49) the other day, and I saw that they too had gone for the cast (`>>> 0`) to turn their values into the more logical `uint32` format. But introducing all those `int32` values all over the place incurs a performance penalty that propagates through any function that handles those values, and in a color manipulation library, that's all of the functions.\n\n\n<RandomPlant className='mt-8 mb-16' />\n\n## 2. Parsing\n\nSatisfied with the color representation, I went on to look at the next most expensive operation in a color library. I won't go into details into formats others than `#rrggbbaa` because for the other ones (`rgba()`, `hsl()`, `color(display-p3, )`, etc), anything other than regex parsing was suboptimal. Yes I tried a standard recursive descent top-down parser, unfortunaly this is javascript, and that sort of work is better left to the engine.\n\nHowever, I had a feeling that hexadecimal parsing in particular could be improved.\n\nLet's take for example a naive regex approach:\n\n```javascript\nconst pattern = /#(..)(..)(..)(..)/\n\nfunction parseHex_regex(color) {\n  const m = color.match(pattern)\n  return (\n    (parseInt(m[1], 16) << 24) |\n    (parseInt(m[2], 16) << 16) |\n    (parseInt(m[3], 16) <<  8) |\n    (parseInt(m[4], 16) <<  0)\n  )\n}\n```\n\nThe problem with such an approach is that we would be creating a lot of allocations, the return value of `RegExp.prototype.match` being quite heavy—it's an array of strings with custom fields. As we've seen in the first benchmark, avoiding object allocations is crucial to maintain a good performance.\n\nSo a good iteration on that to avoid the fat result array would be something like this:\n\n```javascript\nfunction parseHex_slice(color) {\n  return (\n    (parseInt(color.slice(1, 3), 16) << 24) |\n    (parseInt(color.slice(3, 5), 16) << 16) |\n    (parseInt(color.slice(5, 7), 16) <<  8) |\n    (parseInt(color.slice(7, 9), 16) <<  0)\n  )\n}\n```\n\nBut the problem is that even if we got rid of the fat result array, the `.slice()` calls are still allocating 4 strings, and those strings are going to create more GC pauses.\n\nThe only solution to extract characters of a string as numbers rather than strings is `.charCodeAt()`, so that's what I picked to use for my fast parsing implementation. There are two techniques that I explored to turn those characters into their hexadecimal values, both of which I found on [Daniel Lemire's blog](https://lemire.me/blog/2019/04/17/parsing-short-hexadecimal-strings-efficiently/) (where I somehow always end up when researching obscure optimization problems). The first one is to use a mathematical function, and the second one is to use a lookup array. Here is a javascript port of both:\n\n```javascript\n// Transform char code to its hexadecimal value\nfunction x(c) { return (c & 0xf) + 9 * (c >> 6) }\n\nfunction parseHex_function(color) {\n  const r = x(color.charCodeAt(1)) << 4 | x(color.charCodeAt(2))\n  const g = x(color.charCodeAt(3)) << 4 | x(color.charCodeAt(4))\n  const b = x(color.charCodeAt(5)) << 4 | x(color.charCodeAt(6))\n  const a = x(color.charCodeAt(7)) << 4 | x(color.charCodeAt(8))\n  return (\n    (r << 24) |\n    (g << 16) |\n    (b <<  8) |\n    (a <<  0)\n  )\n}\n```\n\n```javascript\n// Transform char code to its hexadecimal value,\n// but as a lookup array.\nconst _ = 0\nconst X = [\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  0,   1, 2,  3,  4,  5,  6,  7,  8,\n 9,  _,  _,  _,  _,  _,  _,  _, 10, 11, 12, 13, 14, 15,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _, 10, 11, 12, 13, 14, 15,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _\n]\n\nfunction parseHex_table(color) {\n  const r = X[color.charCodeAt(1)] << 4 | X[color.charCodeAt(2)]\n  const g = X[color.charCodeAt(3)] << 4 | X[color.charCodeAt(4)]\n  const b = X[color.charCodeAt(5)] << 4 | X[color.charCodeAt(6)]\n  const a = X[color.charCodeAt(7)] << 4 | X[color.charCodeAt(8)]\n  return (\n    (r << 24) |\n    (g << 16) |\n    (b <<  8) |\n    (a <<  0)\n  )\n}\n```\n\nWith all that setup, let's benchmark to see which one is the fastest:\n\n<div id=\"benchmark-parse\" class=\"code-blocks\">\n\n<div class=\"hidden\">\n\n```javascript\n// setup\nconst pattern = /#(..)(..)(..)(..)/\n\nfunction parseHex_regex(color) {\n  const m = color.match(pattern)\n  return (\n    (parseInt(m[1], 16) << 24) |\n    (parseInt(m[2], 16) << 16) |\n    (parseInt(m[3], 16) <<  8) |\n    (parseInt(m[4], 16) <<  0)\n  )\n}\n\nfunction parseHex_slice(color) {\n  return (\n    (parseInt(color.slice(1, 3), 16) << 24) |\n    (parseInt(color.slice(3, 5), 16) << 16) |\n    (parseInt(color.slice(5, 7), 16) <<  8) |\n    (parseInt(color.slice(7, 9), 16) <<  0)\n  )\n}\n\n// Char code to its hex value\nfunction x(c) { return (c & 0xf) + 9 * (c >> 6) }\n\nfunction parseHex_function(color) {\n  const r = x(color.charCodeAt(1)) << 4 | x(color.charCodeAt(2))\n  const g = x(color.charCodeAt(3)) << 4 | x(color.charCodeAt(4))\n  const b = x(color.charCodeAt(5)) << 4 | x(color.charCodeAt(6))\n  const a = x(color.charCodeAt(7)) << 4 | x(color.charCodeAt(8))\n  return (\n    (r << 24) |\n    (g << 16) |\n    (b <<  8) |\n    (a <<  0)\n  )\n}\n\n// Char code to its hex value, but as a table\nconst _ = 0\nconst X = [\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  0,   1, 2,  3,  4,  5,  6,  7,  8,\n 9,  _,  _,  _,  _,  _,  _,  _, 10, 11, 12, 13, 14, 15,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _, 10, 11, 12, 13, 14, 15,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,  _,\n _,  _,  _,  _,  _,  _,  _,  _,  _\n]\n\nfunction parseHex_table(color) {\n  const r = X[color.charCodeAt(1)] << 4 | X[color.charCodeAt(2)]\n  const g = X[color.charCodeAt(3)] << 4 | X[color.charCodeAt(4)]\n  const b = X[color.charCodeAt(5)] << 4 | X[color.charCodeAt(6)]\n  const a = X[color.charCodeAt(7)] << 4 | X[color.charCodeAt(8)]\n  return (\n    (r << 24) |\n    (g << 16) |\n    (b <<  8) |\n    (a <<  0)\n  )\n}\n```\n\n</div>\n\n\n```javascript\n// setup\nconst N = 100_000\nconst colors = new Array(100)\n```\n\n```javascript\n// 1. regex\nfor (let i = 0; i < N; i++) {\n  colors[i % 100] = parseHex_regex('#599eff80')\n}\n```\n\n```javascript\n// 2. slice\nfor (let i = 0; i < N; i++) {\n  colors[i % 100] = parseHex_slice('#599eff80')\n}\n```\n\n```javascript\n// 3. char + function\nfor (let i = 0; i < N; i++) {\n  colors[i % 100] = parseHex_function('#599eff80')\n}\n```\n\n```javascript\n// 4. char + table\nfor (let i = 0; i < N; i++) {\n  colors[i % 100] = parseHex_table('#599eff80')\n}\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-parse\"\n  results={{\"1. regex\":{\"runTime\":-1010,\"amountOfRounds\":91,\"percent\":0.73},\"2. slice\":{\"runTime\":-1003,\"amountOfRounds\":135,\"percent\":1.08},\"3. char + function\":{\"runTime\":-1000,\"amountOfRounds\":12499,\"percent\":100},\"4. char + table\":{\"runTime\":-1000,\"amountOfRounds\":3888,\"percent\":31.11}}}\n  client:load\n/>\n\nAs you can see, both allocation-less versions run *much* faster than the naive ones. Daniel suggests that the table version runs faster than the function version in C, but as we're running inside a javascript engine, our array lookups are considerably more expensive (due to bound checks and whatnot), so the final winner is `parseHex_function`.\n\n<RandomPlant className='mt-8 mb-16' />\n\n## 3. Formatting\n\nAnd for the final part, I looked into how to output those values as strings efficiently. In an ideal setting, color values wouldn't need to be turned back to strings, but this is unavoidable as most APIs javascript interacts with are string-based.\n\nThe simple implementation I went for initially was the following one:\n\n```javascript\nfunction format_simple(color) {\n  return `#${color.toString(16).padStart(8, '0')}`\n}\n```\n\nBut I wasn't happy with the results. One thing I wanted to avoid is the temporary string allocations, the ones returned by `toString` and `padStart`. I also wanted to avoid as much as possible calling javascript string functions like those ones because I usually find that them to be less efficient than pure-javascript ones. It could be for a variety of reasons (e.g. the spec requiring expensive checks/conversions) but I don't know enough about to explain it.\n\nSo I pulled inspiration from the previously presented function from Daniel's blog, and went for a lookup array that converts hexadecimal values to their string representation:\n\n```javascript\n// ['00', '01', ..., 'fe', 'ff']\nconst X =\n  Array.from({ length: 256 })\n    .map((_, i) => i.toString(16).padStart(2, '0'))\n\nfunction format_table(color) {\n  return (\n    '#' +\n    X[color >> 24 & 0xff] +\n    X[color >> 16 & 0xff] +\n    X[color >>  8 & 0xff] +\n    X[color >>  0 & 0xff]\n  )\n}\n```\n\nAnd here is a final benchmark to validate that we're indeed running faster:\n\n<div id=\"benchmark-format\" class=\"code-blocks\">\n\n<div class=\"hidden\">\n\n  ```javascript\n  // setup\n  function format_simple(color) {\n    return `#${color.toString(16).padStart(8, '0')}`\n  }\n\n  // ['00', '01', ..., 'fe', 'ff']\n  const X =\n    Array.from({ length: 256 })\n      .map((_, i) => i.toString(16).padStart(2, '0'))\n\n  function format_table(color) {\n    return (\n      '#' +\n      X[color >> 24 & 0xff] +\n      X[color >> 16 & 0xff] +\n      X[color >>  8 & 0xff] +\n      X[color >>  0 & 0xff]\n    )\n  }\n  ```\n\n</div>\n\n\n```javascript\n// setup\nconst N = 100_000\nconst colors = new Array(100)\n```\n\n```javascript\n// 1. simple\nfor (let i = 0; i < N; i++) {\n  colors[i % 100] = format_simple(0x0f0f0f)\n}\n```\n\n```javascript\n// 2. table\nfor (let i = 0; i < N; i++) {\n  colors[i % 100] = format_table(0x0f0f0f)\n}\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-format\"\n  results={{\"1. simple\":{\"runTime\":-1004,\"amountOfRounds\":210,\"percent\":45.55},\"2. table\":{\"runTime\":-1001,\"amountOfRounds\":461,\"percent\":100}}}\n  client:load\n/>\n\n<RandomPlant className='mt-8 mb-16' />\n\n## 4. Benchmarks\n\nSo with all that completed let's see how it compares to other existing color libraries on NPM. I re-used the benchmarking code from `colord` and slightly modified it to showcase how it performs for the case it was optimized for (hexadecimal strings), so the [benchmark code](https://github.com/romgrk/color-bits/tree/master/benchmarks) consists of parsing a hex color string, modifying its opacity, and converting it back to a hex color string.\n\n| Library        | Operations/sec | Relative speed |\n| ---            | --:            | --:            |\n| **color-bits** | **22 966 299** | fastest        |\n| colord         | 4 308 547      | 81.24% slower  |\n| tinycolor2     | 1 475 762      | 93.57% slower  |\n| chroma-js      | 846 924        | 96.31% slower  |\n| color          | 799 262        | 96.52% slower  |\n\nTada ✨ About 5x faster than the 2nd place, `colord`, which had the previous claim to the fastest color library. I haven't included the benchmark for non `#rrggbbaa` colors like `rgb()` or `hsl()`, but `color-bits` is still around 2x faster than the 2nd place even in those cases.\n\n<RandomPlant className='mt-8 mb-16' />\n\n## 5. Closing thoughts\n\nI think avoiding memory allocations is one of the **easiest and most impactful** ways to speed up a program. Javascript is a very convenient language, allocating a new object can be as simple as typing `{}`, `[]` or `{ ...newObject }`, and I love the expressivity it brings. But it's a double-edged sword, because it makes those memory allocations less apparent. It's also hard to notice how those allocations impact your program, because allocating memory is somewhat fast, but managing and freeing allocations are the actually expensive operations. And that will appear as a single blob of `Minor GC` or `Major GC` entries in your stack traces. Most of the time, when I profile a program, `Minor GC` is in the top 5 entries! But it's so easy to just dimiss it as \"stuff the engine needs to do anyway\". That's not the case. **Any garbage to collect is garbage you have created**. If you want to build fast, responsive, delightful software, you need to architect it to avoid creating and re-creating objects constantly like some frameworks do (yes, I'm looking at you React). Pick instead proper frameworks and solutions that are built with good fundamentals (why won't the world adopt SolidJS already?).\n\nI hope this wasn't too boring. As always, feel free to email any comments, corrections or questions, link in the footer.\n\nYou can follow me on [github](https://github.com/romgrk) or subscribe to the RSS feed if you want to see more performance-related stuff.\n\nNPM: https://www.npmjs.com/package/color-bits  \nGithub: https://github.com/romgrk/color-bits  \n\n<RandomPlant className='mt-8 mb-16' />","src/content/posts/color-bits.mdx","29ac20d67b082ebe","color-bits.mdx","efficient-typescript",{id:37,data:39,body:43,filePath:44,digest:45,legacyId:46,deferredRender:19},{title:40,description:28,pubDate:41,sidebar:42},"Efficient Typescript","Oct 20 2024",{display:18},"import Aside from '../../components/Aside.astro'\nimport RandomPlant from '../../components/client/RandomPlant/RandomPlant.tsx'\nimport exampleSuccess from './efficient-typescript/typescript-success.png'\nimport exampleError from './efficient-typescript/typescript-error.png'\n\n\nOne of the most useful ways that a type-system can work is by preventing a class of errors. The simplest way to interpret that is that it will prevent you from using a `number` where you need a `string`, but there's more to it. Typescript allows you to encode more complex restrictions into the type-system, so the compiler can help you avoid making some mistakes.\n\nIt's an issue I see frequently, even in popular library code. For example, let's take this snippet from `react-query`'s documentation:\n\n```javascript\nconst { isPending, error, data } = useQuery({\n  queryKey: ['repoData'],\n  queryFn: async () => fetch(\n    'https://api.github.com/repos/TanStack/query'\n  ),\n});\n\nif (isPending) return 'Loading...'\n\nif (error) return 'Error: ' + error.message\n\nreturn (\n  <div>\n    <h1>{data.full_name}</h1>\n    <p>{data.description}</p>\n  </div>\n)\n```\n\nThere is here an implicit constraint that if `isPending` or `error` is present, then `data` isn't. But ensuring that constraint is a task that's left to you, the fallible programmer. Typescript actually allows you to **turn that implicit constraints into an explicit one**, which means the compiler could be doing work so that you don't have to think about it.\n\nFor example, if the typings were defined as such:\n\n```typescript\ntype PendingState   = { isPending: true,  error: null,  data: null }\ntype ErrorState     = { isPending: false, error: Error, data: null }\ntype LoadedState<T> = { isPending: false, error: null,  data: T }\n\ntype QueryResult<T> = PendingState | ErrorState | LoadedState<T>\n```\n\nThen the compiler would have enough information to tell you that you can't use `data` if it is `null`:\n\n<div class=\"flex flex-col md:flex-row w-full\">\n  <img class=\"w-full md:w-1/2\" src={exampleSuccess.src} />\n  <img class=\"w-full md:w-1/2\" src={exampleError.src} />\n</div>\n\n<div class=\"text-center text-sm\">\n  [See this example in Typescript playground](https://www.typescriptlang.org/play/?#code/FAFwngDgpgBAClAdgEwJaIOYGUQEMSwwwC8MA3jKgM4IroYBcMIATgK5QA0RULLA9iyaI2AG1HcYyfLmFjRMAL6hIsAKJ9BOfISIlylGkjSYmAM1yiqXGLwFCYG+92l454pSugwAMv1zIUMjaBAA8ACoAfPoU1LQmjDAWVjZ2gu4SRK6yMOGeXrAAihwsYABKUFRiIBHRpPH0IbAAPo6aLE0wrX4BQU21wMBmbIgAxiCo-IgwbNbFvGAAFACU5MBELFAgbCzTsUZ0pszsqe0ZLjIZSjC4VDDzpRVVojUUiLgAtlBMVKz0SpFgMohiNxpNpgBhfgfCBTJAgFZrIijKa-AxxYz0bhpFgXPDXUizKAPJbLADcgyIAHoqZQzDBFhjDhhVpttrsYAByHoJAB0-M5lLpDJxrK2O2mnKc6S5MAA1LZ2ryvlQqLgMFAhWyJQz1npsrz3l89csgUA)\n</div>\n\nThis is essentially an implementation of the age-old programming saying, **\"make illegal state unrepresentable\"**.\n\n<Aside title=\"Correction\">\n  Some commenter pointed out that react-query is a bad example because it can handle more complex situations with its different flags (notably keep `.data` alive while re-fetching). I'll leave the example as is, because the example is good, even if contrived.  \n  The library could also split that behavior into a separate `useRetainedData` hook, which would allow for the base `useQuery` hook to have strong type constraints.\n</Aside>\n\nProper error handling was pioneered by monadic languages such as Haskell, which has [the `Either` monad](https://hackage.haskell.org/package/base-4.20.0.1/docs/Data-Either.html) and its two subtypes, `Left` and `Right`. A more pragmatic approach exists in Rust, which has [the `Result` type](https://doc.rust-lang.org/std/result/) and its two subtypes `Ok` and `Err`. Those are lessons that we should apply to modern programming to get both safe and ergonomic error handling.\n\nOne example of ergonomic error handling: `Result` being a Functor means we can implement it as a class with a `.map` method, so we could transform the value inside a `Result` the same way we can operate on values inside an `Array` (`Ok` would apply the mapping function, while mapping an `Err` would be a no-op).\n\n```typescript\nfunction findUser(): Result<User>;\n\nconst name = findUser().map(user => user.name) // Result<string>\n```\n\nThis allows you to delay error handling for later, while still allowing you to transform the value. It's very similar to how `Promise` let's you chain `.then` calls even if you haven't attached a `.catch` handler yet. In fact, the cases are related because `Promise` is basically a monad, bar a few minor details.\n\nOther methods from category theory would also be applicable, such as `.flatMap()` or `.fold()`, which in turn allow new expressive ways to write code:\n\n```typescript\nconst result = useQuery()\n\nreturn result.fold(\n  () => <Spinner />,\n  error => <Message error={error} />,\n  data => <p>{data.name}</p>,\n)\n```\n\nBut the greatest benefit of this kind of error-handling is that it **encodes failure directly in the type-system**, so you know at compile-time if you haven't dealt with a failure, rather than have to wait to `catch` errors at runtime.\n\n## Opaque types\n\nOne feature that I really miss in Typescript compared to type-systems not based on structural typings is being able to declare new types that can hold the same value type without being able to mix them up. Here is an easy example:\n\n```typescript\ntype Width  = number\ntype Height = number\n\nconst baseHeight = 50  as Height\nconst baseWidth  = 100 as Width\n\nconst sectionWidth = 40 as Width\nconst diff  = baseWidth - sectionWidth\n//    ^ is of type `Width`\n\n// ...then later in the code\nconst newValue = baseHeight - diff\n// Error, we are mixing Height and Width!\n// But TS won't warn you :(\n```\n\nHow wonderful would it be if the compiler could prevent us from mixing values that shouldn't be mixed! This is a constraint that we'll often try to encode in the variable name (e.g. all width variables ending with `...Width`), but it's a shame that the type-system can't eliminate this type of error. It is possible to [create somewhat opaque types](https://calebmer.com/tangents/opaque-types-in-typescript.html) that can't be mixed, but because we can't express which operators are valid for a type, the compiler would consider `Width + Width` as an error if it was defined as such. So most of the time, using opaque types to add constraints is not worth the hassle.\n\nOne notable exception could be safe/unsafe strings. For example, if you're building a templating engine and need to pass user-input strings, being able to have a `SafeString` type could allow the compiler to ensure safety:\n\n```typescript\ndeclare const __safe: unique symbol;\ntype SafeString = string & { [__safe]: true };\n\nfunction escape(value: string): SafeString {\n  /* ... */\n}\n\nfunction format(template: string, values: Record<string, SafeString>) {\n  /* ... */\n}\n\n// then use it as such\nformat(\n  \"<html><p>User name: %name%</p></html>\",\n  {\n    name: \"Unsafe! <script src=\\\"https://attacker.com/script.js\\\" />\"\n    //    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    //    Type 'string' is not assignable to type 'SafeString'.\n  }\n)\n```\n\n## Conclusion\n\nPlease, add explicit constraints to your APIs. **Well placed constraints are not limitations, they are actually freedom**. It lets the compiler do the dirty work of checking all the minor details your overlooked, and let's you build more reliable software.\n\nTypescript also has much more depth than what I covered here, so if you want to understand how to encode more complex constraints, I highly recommend reading these posts:\n\nhttps://zhenghao.io/posts/ts-never  \nhttps://zhenghao.io/posts/type-programming  \nhttps://zhenghao.io/posts/type-functions  \n\n<RandomPlant className='mt-8 mb-16' />","src/content/posts/efficient-typescript.mdx","261a9dc2d653c341","efficient-typescript.mdx","react-fast-memo",{id:47,data:49,body:53,filePath:54,digest:55,legacyId:56,deferredRender:19},{title:50,description:28,pubDate:51,rating:52},"A faster React.memo()","Mar 8 2024",6,"import RandomPlant from '../../components/client/RandomPlant/RandomPlant.tsx'\n\n<small>\n[*\"Just show me the code\"*: click here](https://github.com/romgrk/react-fast-memo)\n</small>\n\nI recently spent time optimizing React code, and the obvious answer is, as always, `React.memo()`. So to speed things up I\nadded a bunch of them everywhere because [you should memo all the things by default](https://attardi.org/why-we-memo-all-the-things/)\nanyway. This lead me to think if there was a way to make `React.memo()` faster. Not that it needs it, I just enjoy a bit\ntoo much performance. And the answer is yes.\n\nAnd with the compiler not being shipped in React 19, it seems like we're going to have to keep handling our own\nmemoization for a bit longer, so I might as well share this one with you.\n\n### The Code\n\nSo to make memo faster (whose signature is `React.memo(c: Component, f: CompareFunction)`, btw), we need to write a faster\ncompare function than the one React has. The good news here is that React being a generic framework kinda needs to expect\nits users to do all sorts of funky stuff, whereas we, my friend, can tell our users to just not do anything funky because\nwe won't support that.\n\nSo let's start with React's implementation, and let's see how we can make it faster.\n\n```javascript\nfunction shallowEqual(objA, objB) {\n  if (Object.is(objA, objB)) { return true; }\n\n  if (typeof objA !== 'object' || objA === null ||\n      typeof objB !== 'object' || objB === null) {\n    return false;\n  }\n\n  const keysA = Object.keys(objA);\n  const keysB = Object.keys(objB);\n\n  if (keysA.length !== keysB.length) {\n    return false;\n  }\n\n  for (let i = 0; i < keysA.length; i++) {\n    const currentKey = keysA[i];\n    if (\n      !hasOwnProperty.call(objB, currentKey) ||\n      !is(objA[currentKey], objB[currentKey])\n    ) {\n      return false;\n    }\n  }\n\n  return true;\n}\n```\n\nThe first thing I don't like with this approach is the usage of `Object.keys()`. If we're going to be calling this\nfunction very often, allocating 2 new arrays on each call is nuts. It's the easy way, but it's also the wrong way if the\ngoal is performance (not that it should always be, most of the times, it's readability). Whatever software you're\nwriting, unless you're dealing with network requests, memory IO is always going to be the biggest cost. Allocating arrays\nin RAM is just one example of that. So let's try to get rid of that.\n\n\nAnother section that I'm not a fan of is this prelude.\n\n```javascript\nif (Object.is(objA, objB)) { return true; }\n\nif (typeof objA !== 'object' || objA === null ||\n    typeof objB !== 'object' || objB === null) {\n  return false;\n}\n```\n\nWhat I think happened here is they implemented a generic \"shallow equals\" function, and re-used it where needed, one\nplace being `React.memo()`. One of the advantages that we have here is that we're optimizing for one case, and we should\ntherefore be specializing this function for React props. So let us assume these properties about React props:\n - They don't have funky prototype chains.\n - They are not the same object: prop objects are always inline objects.\n - They are likely to be the same shape (monomorphic).\n - They are likely to be equal: most of the time, only a small fraction of the UI changes.\n - ⚠️ They are not `null` and always an object: `<div />` is transformed to `_jsx('div', {})`.\n - ⚠️ Having `prop={undefined}` or no prop is functionally equivalent.\n\nI have marked with ⚠️  the assumptions that are unsafe and would lead to an incorrect \"generic shallow compare\" function,\nand we'll come back to these later. With that in mind, here is the implementation I landed on:\n\n```javascript\nexport function fastCompareUnsafe(a, b) {\n  let aLength = 0;\n  let bLength = 0;\n\n  for (const key in a) {\n    aLength += 1;\n\n    if (!Object.is(a[key], b[key])) {\n      return false;\n    }\n  }\n\n  for (const _ in b) {\n    bLength += 1;\n  }\n\n  return aLength === bLength;\n}\n```\n\nAs we can assume non-nullable objects, we were able to get rid of the prelude. Comparing the key count to check for equality\nis nice, but to avoid using `Object.keys()` like in the original we had to do it by iterating each key of the object. You\nmight be tempted to propose that we return early in the last loop instead of just counting the keys:\n\n```javascript\n  for (const _ in b) {\n    bLength += 1;\n\n    // Why not return?\n    // if (b[_] != a[_]) {\n    //   return false\n    // }\n  }\n```\n\nThe problem problem with that is memory IO again: checking `a[key]` loads the value from memory, which is expensive. By\navoiding the early return there, we can avoid touching `a` at all again. Depending on which JS engine we're running on,\nwe might even not be touching `b` again even if we're iterating its keys, because most engines keep separate memory tables\nfor an object keys and its values.\n\nOne other interesting change we were able to do, because we assumed that \"they don't have funky prototype chains\", is we\nwere able to remove the `Object.hasOwnProperty()` call that React has:\n\n```javascript {4}\n  for (let i = 0; i < keysA.length; i++) {\n    const currentKey = keysA[i];\n    if (\n      !hasOwnProperty.call(objB, currentKey) ||\n      !is(objA[currentKey], objB[currentKey])\n    ) {\n      return false;\n    }\n  }\n```\n\nI'll note that V8 and JavaScriptCore are able to optimize away the `hasOwnProperty` call, but SpiderMonkey can't (yet),\nso this optimization is Firefox specific.\n\n## Safe version\n\nSome of the assumptions we made above are unsafe however, in particular if you use the pattern `'key' in props` in your\ncodebase. So here is also the function without the unsafe assumptions, where we have added back the prelude and the check\nfor the same keys including `undefined` ones:\n\n```javascript {2-7} {18-20}\nexport function fastCompare(a, b) {\n  if (a === b) {\n    return true;\n  }\n  if (!(a instanceof Object) || !(b instanceof Object)) {\n    return false;\n  }\n\n  let aLength = 0;\n  let bLength = 0;\n\n  for (const key in a) {\n    aLength += 1;\n\n    if (!Object.is(a[key], b[key])) {\n      return false;\n    }\n    if (!(key in b)) {\n      return false;\n    }\n  }\n\n  for (const _ in b) {\n    bLength += 1;\n  }\n\n  return aLength === bLength;\n}\n```\n\n\n## How fast are we?\n\nI've benchmarked against a bunch of existing modules, it seems like we're the fastest<sup>1,</sup><sup>2</sup>! There are\nvarious numbers in the table below, but the important part is that we're about 2x faster than before.\n\nI have compared 2 implementations, one where we don't include the ⚠️  assumptions, and another one where we do.\n\n```jsonc\n{\n  \"fbjs/shallowEqual:equal\":                 { t: 1322.67, stddev: 1.69 },\n  \"fbjs/shallowEqual:unequal\":               { t: 1243.67, stddev: 1.24 },\n\n  \"fast-shallow-equal:equal\":                { t: 1235.67, stddev: 36.80 },\n  \"fast-shallow-equal:unequal\":              { t: 1241.33, stddev: 1.69 },\n\n  \"react:equal\":                             { t: 1261,    stddev: 3.74 },\n  \"react:unequal\":                           { t: 1249,    stddev: 1.63 },\n\n  \"shallowequal:equal\":                      { t: 1172.67, stddev: 71.94 },\n  \"shallowequal:unequal\":                    { t: 1194.33, stddev: 30.15 },\n\n  \"fast-equals.shallowEqual:equal\":          { t: 1237.67, stddev: 27.35 },\n  \"fast-equals.shallowEqual:unequal\":        { t: 325.67,  stddev: 1.88 },\n  // ^ this one is surprisingly fast, but only for the \"unequal objects\"\n  //   case which is the unlikely one, so we're not really interested in it.\n\n  // Safe: does not include assumptions\n  \"romgrk-fastCompare:equal\":                { t: 871.67,  stddev: 11.58 },\n  \"romgrk-fastCompare:unequal\":              { t: 777,     stddev: 8.83 },\n\n  \"hughsk/shallow-equals:equal\":             { t: 600.67,  stddev: 35.31 },\n  \"hughsk/shallow-equals:unequal\":           { t: 562.67,  stddev: 3.68 },\n  // ^ this one is pretty close to our implementation above! But it uses\n  //   `===` for comparison instead of `Object.is`, which always returns\n  //   false for `NaN === NaN`\n\n  // Unsafe: includes ⚠️ assumptions\n  \"romgrk-fastCompareUnsafe:equal\":          { t: 515,     stddev: 7.48 },\n  \"romgrk-fastCompareUnsafe:unequal\":        { t: 445.33,  stddev: 1.24 },\n}\n```\n\n<sup>1</sup> Alright ok but we do need to admit that we've optimized for \"react props\", not for general correctness. We\nreturn `true` for `equals({ a: 1, b: undefined }, { a: 1, c: undefined })`, which works for us but not for a generic compare\nfunction. <br />\nWe do have `fastCompare` which is still the fastest and 1.5x faster than react if we only consider candidates that use safe\nassumptions. `hughsk/shallow-equals` is discarded due to its `===` use.\n\n<sup>2</sup> Alright and yes, I have excluded one package that was faster from this list. In my defense, it was [shallow-equal-jit](https://www.npmjs.com/package/shallow-equal-jit),\nwhich requires you to pass the object keys beforehand and only works if the keys stay the same, which does not work for\nreact props. We can assume the keys are *likely* to be the same, but we can't assume they are going to be the same.\n\n## Final notes\n\nSo anyway, if you're not doing anything funky with your React props and are really way too interested in making your app\nperformant, you can [checkout the repo](https://github.com/romgrk/react-fast-memo)\nor `pnpm install react-fast-memo`.\n\nThe unsafe version might seem unuseful, but it has certain niche use-cases: if you're comparing objects that you can 100%\nguarante have the same shape, then that one should be your pick.\n\nOne big asterisk though: on JavaScriptCore, the unsafe version is slower! Up-to-date benchmarks [here](https://github.com/romgrk/react-fast-memo?tab=readme-ov-file#benchmarks).\n\n<br />\n<br />\n<br />\n\n<RandomPlant />\n\n<br />\n<br />\n\n## Other notes\n\n*\"It doesn't matter for 99% of use-cases\"*: Yes you're right.","src/content/posts/react-fast-memo.mdx","e464b8e3bb2f7752","react-fast-memo.mdx","optimizing-javascript",{id:57,data:59,body:65,filePath:66,digest:67,legacyId:68,deferredRender:19},{title:60,description:61,pubDate:62,sidebar:63,rating:64},"Optimizing Javascript for fun and for profit","Or how to write performant javascript","Mar 21 2024",{depth:31},10,"import Aside from '../../components/Aside.astro'\nimport RandomPlant from '../../components/client/RandomPlant/RandomPlant.tsx'\nimport Benchmark from '../../components/client/Benchmark.tsx'\n\n\nI often feel like javascript code in general runs much slower than it could, simply because it's not optimized properly. Here is a summary of common optimization techniques I've found useful. Note that the tradeoff for performance is often readability, so the question of when to go for performance versus readability is a question left to the reader. I'll also note that talking about optimization necessarily requires talking about benchmarking. Micro-optimizing a function for hours to have it run 100x faster is meaningless if the function only represented a fraction of the actual overall runtime to start with. If one is optimizing, the first and most important step is benchmarking. I'll cover the topic in the later points. Be also aware that micro-benchmarks are often flawed, and that may include those presented here. I've done my best to avoid those traps, but don't blindly apply any of the points presented here without benchmarking.\n\nI have included runnable examples for all cases where it's possible. They show by default the results I got on my machine (brave 122 on archlinux) but you can run them yourself. As much as I hate to say it, Firefox has fallen a bit behind in the optimization game, and represents a very small fraction of the traffic [for now](https://foundation.mozilla.org/en/?form=donate-header), so I don't recommend using the results you'd get on Firefox as useful indicators.\n\n## 0. Avoid work\n\nThis might sound evident, but it needs to be here because there can't be another first step to optimization: if you're trying to optimize, you should first look into avoiding work. This includes concepts like memoization, laziness and incremental computation. This would be applied differently depending on the context. In React, for example, that would mean applying `memo()`, `useMemo()` and other applicable primitives.\n\n## 1. Avoid string comparisons\n\nJavascript makes it easy to hide the real cost of string comparisons. If you need to compare strings in C, you'd use the `strcmp(a, b)` function. Javascript uses `===` instead, so you don't see the `strcmp`. But it's there, and a string comparison will usually (but not always) require comparing each of the characters in the string with the ones in the other string; string comparison is `O(n)`. One common JavaScript pattern to avoid is strings-as-enums. But with the advent of TypeScript this should be easily avoidable, as\nenums are integers by default.\n\n<div class=\"code-blocks-row\">\n\n```typescript\n// No\nenum Position {\n  TOP    = 'TOP',\n  BOTTOM = 'BOTTOM',\n}\n```\n\n```typescript\n// Yeppers\nenum Position {\n  TOP,    // = 0\n  BOTTOM, // = 1\n}\n```\n\n</div>\n\nHere is a comparison of the costs:\n\n<div id=\"benchmark-compare\" class=\"code-blocks-row\">\n\n```javascript\n// 1. string compare\nconst Position = {\n  TOP: 'TOP',\n  BOTTOM: 'BOTTOM',\n}\n\nlet _ = 0\nfor (let i = 0; i < 1000000; i++) {\n  let current = i % 2 === 0 ?\n    Position.TOP : Position.BOTTOM\n  if (current === Position.TOP)\n    _ += 1\n}\n```\n\n```javascript\n// 2. int compare\nconst Position = {\n  TOP: 0,\n  BOTTOM: 1,\n}\n\nlet _ = 0\nfor (let i = 0; i < 1000000; i++) {\n  let current = i % 2 === 0 ?\n    Position.TOP : Position.BOTTOM\n  if (current === Position.TOP)\n    _ += 1\n}\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-compare\"\n  results={{\"1. string compare\":{\"runTime\":-1000,\"amountOfRounds\":828,\"percent\":50.35},\"2. int compare\":{\"runTime\":-1000,\"amountOfRounds\":2137,\"percent\":100}}}\n  client:load\n/>\n\n<Aside title=\"About benchmarks\" small>\n  Percentage results represent the number of operations completed within 1s, divided by the number of operations of the highest scoring case. Higher is better.\n</Aside>\n\nAs you can see, the difference can be significant. The difference isn't necessarily due to the `strcmp` cost as engines can sometimes use a string pool and compare by reference, but it's also due to the fact that integers are usually passed by value in JS engines, whereas strings are always passed as pointers, and memory accesses are expensive (see section 5). In string-heavy code, this can have a huge impact.\n\nFor a real-world example, I was able to [make this JSON5 javascript parser run 2x faster](https://github.com/json5/json5/pull/278)* just by replacing string constants with numbers.  \n<small>*Unfortunately it wasn't merged, but that's how open-source is.</small>\n\n## 2. Avoid different shapes\n\nJavascript engines try to optimize code by assuming that objects have a specific shape, and that functions will receive objects of the same shape. This allows them to store the keys of the shape once for all objects of that shape, and the values in a separate flat array. To represent it in javascript:\n\n<div class=\"code-blocks-row\">\n\n```javascript\nconst objects = [\n  {\n    name: 'Anthony',\n    age: 36,\n  },\n  {\n    name: 'Eckhart',\n    age: 42\n  },\n]\n```\n\n```javascript\nconst shape = [\n  { name: 'name', type: 'string' },\n  { name: 'age',  type: 'integer' },\n]\n\nconst objects = [\n  ['Anthony', 36],\n  ['Eckhart', 42],\n]\n\n```\n\n</div>\n\n<Aside title=\"A note on terminology\">\n  I have used the word \"shape\" for this concept, but be aware that you may also find *\"hidden class\"* or *\"map\"* used to describe it, depending on the engine.\n</Aside>\n\nFor example, at runtime if the following function receives two objects with the shape `{ x: number, y: number }`, the engine is going to speculate that future objects will have the same shape, and generate machine code optimized for that shape.\n\n```javascript\nfunction add(a, b) {\n  return {\n    x: a.x + b.x,\n    y: a.y + b.y,\n  }\n}\n```\n\nIf one would instead pass an object not with the shape `{ x, y }` but with the shape `{ y, x }`, the engine would need to undo its speculation and the function would suddenly become considerably slower. I'm going to limit my explanation here because you should read  the [excellent post from mraleph](https://mrale.ph/blog/2015/01/11/whats-up-with-monomorphism.html) if you want more details, but I'm going to highlight that V8 in particular has 3 modes, for accesses that are: monomorphic (1 shape), polymorphic (2-4 shapes), and megamorphic (5+ shapes). Let's say you *really* want to stay monomorphic, because the slowdown is drastic:\n\n<div id=\"benchmark-shape\" class=\"code-blocks\">\n\n```javascript\n// setup\nlet _ = 0\n```\n\n```javascript\n// 1. monomorphic\nconst o1 = { a: 1, b: _, c: _, d: _, e: _ }\nconst o2 = { a: 1, b: _, c: _, d: _, e: _ }\nconst o3 = { a: 1, b: _, c: _, d: _, e: _ }\nconst o4 = { a: 1, b: _, c: _, d: _, e: _ }\nconst o5 = { a: 1, b: _, c: _, d: _, e: _ } // all shapes are equal\n```\n\n```javascript\n// 2. polymorphic\nconst o1 = { a: 1, b: _, c: _, d: _, e: _ }\nconst o2 = { a: 1, b: _, c: _, d: _, e: _ }\nconst o3 = { a: 1, b: _, c: _, d: _, e: _ }\nconst o4 = { a: 1, b: _, c: _, d: _, e: _ }\nconst o5 = { b: _, a: 1, c: _, d: _, e: _ } // this shape is different\n```\n\n```javascript\n// 3. megamorphic\nconst o1 = { a: 1, b: _, c: _, d: _, e: _ }\nconst o2 = { b: _, a: 1, c: _, d: _, e: _ }\nconst o3 = { b: _, c: _, a: 1, d: _, e: _ }\nconst o4 = { b: _, c: _, d: _, a: 1, e: _ }\nconst o5 = { b: _, c: _, d: _, e: _, a: 1 } // all shapes are different\n```\n\n```javascript\n// test case\nfunction add(a1, b1) {\n  return a1.a + a1.b + a1.c + a1.d + a1.e +\n         b1.a + b1.b + b1.c + b1.d + b1.e }\n\nlet result = 0\nfor (let i = 0; i < 1000000; i++) {\n  result += add(o1, o2)\n  result += add(o3, o4)\n  result += add(o4, o5)\n}\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-shape\"\n  results={{\"1. monomorphic\":{\"runTime\":-1000,\"amountOfRounds\":1247,\"percent\":100},\"2. polymorphic\":{\"runTime\":-1003,\"amountOfRounds\":163,\"percent\":13.07},\"3. megamorphic\":{\"runTime\":-1008,\"amountOfRounds\":51,\"percent\":4.09}}}\n  client:load\n/>\n\n#### What the eff should I do about this?\n\nEasier said than done but: **create all your objects with the exact same shape**. Even something as trivial as **writing your React component props in a different order can trigger this**.  \n\nFor example, here are [simple cases](https://github.com/facebook/react/pull/28569) I found in React's codebase, but they already had a [much higher impact case](https://v8.dev/blog/react-cliff) of the same problem a few years ago because they were initializing an object with an integer, then later storing a float. Yes, changing the type also changes the shape. Yes, there are integer and float types hidden behind `number`. Deal with it.\n\n<Aside title=\"Number representation\">\n Engines can usually encode integers as values. For example, V8 represents values in 32 bits, with integers as compact [Smi](https://medium.com/fhinkel/v8-internals-how-small-is-a-small-integer-e0badc18b6da) (SMall Integer) values, but floats and large integers are passed as pointers just like strings and objects. JSC uses a 64 bit encoding, [double tagging](https://ktln2.org/2020/08/25/javascriptcore/), to pass all numbers by value, as SpiderMonkey does, and the rest is passed as pointers.\n</Aside>\n\n\n## 3. Avoid array/object methods\n\nI love functional programming as much as anyone else, but unless you're working in Haskell/OCaml/Rust where functional code gets compiled to efficient machine code, functional will always be slower than imperative.\n\n```javascript\nconst result =\n  [1.5, 3.5, 5.0]\n    .map(n => Math.round(n))\n    .filter(n => n % 2 === 0)\n    .reduce((a, n) => a + n, 0)\n```\n\nThe problem with those methods is that:\n\n  1. They need to make a full copy of the array, and those copies later need to be freed by the garbage collector.  We will explore more in details the issues of memory I/O in section 5.\n  2. They loop N times for N operations, whereas a `for` loop would allow looping once.\n\n\n\n<div id=\"benchmark-array-methods\" class=\"code-blocks\">\n\n```javascript\n// setup:\nconst numbers = Array.from({ length: 10_000 }).map(() => Math.random())\n```\n\n```javascript\n// 1. functional\nconst result =\n  numbers\n    .map(n => Math.round(n * 10))\n    .filter(n => n % 2 === 0)\n    .reduce((a, n) => a + n, 0)\n```\n\n```javascript\n// 2. imperative\nlet result = 0\nfor (let i = 0; i < numbers.length; i++) {\n  let n = Math.round(numbers[i] * 10)\n  if (n % 2 !== 0) continue\n  result = result + n\n}\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-array-methods\"\n  results={{\"1. functional\":{\"runTime\":-1002,\"amountOfRounds\":303,\"percent\":36.51},\"2. imperative\":{\"runTime\":-1000,\"amountOfRounds\":830,\"percent\":100}}}\n  client:load\n/>\n\n<br />\n\nObject methods such as `Object.values()`, `Object.keys()` and `Object.entries()` suffer from similar problems, as they also allocate more data, and memory accesses are the root of all performance issues. No really I swear, I'll show you in section 5.\n\n## 4. Avoid indirection\n\nAnother place to look for optimization gains is any source of indirection, of which I can see 3 main sources:\n\n```javascript\nconst point = { x: 10, y: 20 }\n\n// 1.\n// Proxy objects are harder to optimize because their get/set function might\n// be running custom logic, so engines can't make their usual assumptions.\nconst proxy = new Proxy(point, { get: (t, k) => { return t[k] } })\n// Some engines can make proxy costs disappear, but those optimizations are\n// expensive to make and can break easily.\nconst x = proxy.x\n\n// 2.\n// Usually ignored, but accessing an object via `.` or `[]` is also an\n// indirection. In easy cases, the engine may very well be able to optimize the\n// cost away:\nconst x = point.x\n// But each additional access multiplies the cost, and makes it harder for the\n// engine to make assumptions about the state of `point`:\nconst x = this.state.circle.center.point.x\n\n// 3.\n// And finally, function calls can also have a cost. Engine are generally good\n// at inlining these:\nfunction getX(p) { return p.x }\nconst x = getX(p)\n// But it's not guaranteed that they can. In particular if the function call\n// isn't from a static function but comes from e.g. an argument:\nfunction Component({ point, getX }) {\n  return getX(point)\n}\n```\n\nThe proxy benchmark is particularly brutal on V8 at the moment. Last time I checked, proxy objects were always falling back from the JIT to the interpreter, seeing from those results it might still be the case.\n\n<div id=\"benchmark-proxy\" class=\"code-blocks\">\n\n\n```javascript\n// 1. proxy access\nconst point = new Proxy({ x: 10, y: 20 }, { get: (t, k) => t[k] })\n\nfor (let _ = 0, i = 0; i < 100_000; i++) { _ += point.x }\n```\n\n```javascript\n// 2. direct access\nconst point = { x: 10, y: 20 }\nconst x = point.x\n\nfor (let _ = 0, i = 0; i < 100_000; i++) { _ += x }\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-proxy\"\n  results={{\"1. proxy access\":{\"runTime\":-1000,\"amountOfRounds\":659,\"percent\":2.8},\"2. direct access\":{\"runTime\":-1000,\"amountOfRounds\":23544,\"percent\":100}}}\n  client:load\n/>\n\n<div id=\"benchmark-object\" class=\"code-blocks\">\n\nI also wanted to showcase accessing a deeply nested object vs direct access, but engines are very good at [optimizing away object accesses via escape analysis](https://youtu.be/KiWEWLwQ3oI?t=1055) when there is a hot loop and a constant object. I inserted a bit of indirection to prevent it.\n\n```javascript\n// 1. nested access\nconst a = { state: { center: { point: { x: 10, y: 20 } } } }\nconst b = { state: { center: { point: { x: 10, y: 20 } } } }\nconst get = (i) => i % 2 ? a : b\n\nlet result = 0\nfor (let i = 0; i < 100_000; i++) {\n  result = result + get(i).state.center.point.x }\n```\n\n```javascript\n// 2. direct access\nconst a = { x: 10, y: 20 }.x\nconst b = { x: 10, y: 20 }.x\nconst get = (i) => i % 2 ? a : b\n\nlet result = 0\nfor (let i = 0; i < 100_000; i++) {\n  result = result + get(i) }\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-object\"\n  results={{\"1. nested access\":{\"runTime\":-1000,\"amountOfRounds\":9871,\"percent\":42.08},\"2. direct access\":{\"runTime\":-1000,\"amountOfRounds\":23460,\"percent\":100}}}\n  client:load\n/>\n\n\n## 5. Avoid cache misses\n\nThis point requires a bit of low-level knowledge, but has implications even in javascript, so I'll explain. From the CPU point of view, retrieving memory from RAM is slow. To speed things up, it uses mainly two optimizations.\n\n### 5.1 Prefetching\n\nThe first one is prefetching: it fetches more memory ahead of time, in the hope that it's the memory you'll be interested in. It always guesses that if you request one memory address, you'll be interested in the memory region that comes right after that. So **accessing data sequentially** is the key. In the following example, we can observe the impact of accessing memory in random order.\n\n<div id=\"benchmark-prefetch\" class=\"code-blocks\">\n\n<div class=\"hidden\">\n\n```javascript\n// setup:\nfunction shuffle(array) {\n  let currentIndex = array.length,  randomIndex;\n\n  while (currentIndex > 0) {\n    randomIndex = Math.floor(Math.random() * currentIndex);\n    currentIndex--;\n    [array[currentIndex], array[randomIndex]] = [\n      array[randomIndex], array[currentIndex]];\n  }\n\n  return array;\n}\n```\n\n</div>\n\n```javascript\n// setup:\nconst K = 1024\nconst length = 1 * K * K\n\n// Theses points are created one after the other, so they are allocated\n// sequentially in memory.\nconst points = new Array(length)\nfor (let i = 0; i < points.length; i++) {\n  points[i] = { x: 42, y: 0 }\n}\n\n// This array contains the *same data* as above, but shuffled randomly.\nconst shuffledPoints = shuffle(points.slice())\n```\n\n```javascript\n// 1. sequential\nlet _ = 0\nfor (let i = 0; i < points.length; i++) { _ += points[i].x }\n```\n\n```javascript\n// 2. random\nlet _ = 0\nfor (let i = 0; i < shuffledPoints.length; i++) { _ += shuffledPoints[i].x }\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-prefetch\"\n  results={{\"2. random\":{\"runTime\":-1000,\"amountOfRounds\":226,\"percent\":26.22},\"1. sequential\":{\"runTime\":-1000,\"amountOfRounds\":862,\"percent\":100}}}\n  client:load\n/>\n\n#### What the eff should I do about this?\n\nThis aspect is probably the hardest to put in practice, because javascript doesn't have a way of placing objects in memory, but you can use that knowledge to your advantage as in the example above, for example to operate on data before re-ordering or sorting it. You cannot assume that objects created sequentially will stay at the same location after some time because the garbage collector might move them around. There is one exception to that, and it's arrays of numbers, preferably `TypedArray` instances:\n\n```javascript\n// from this\nconst points = [{ x: 0, y: 5 }, { x: 0, y: 10 }]\n\n// to this\nconst points = new Int64Array([0, 5, 0, 10])\n```\n\nFor a more detailed example, [see this link](https://mrale.ph/blog/2018/02/03/maybe-you-dont-need-rust-to-speed-up-your-js.html#optimizing-parsing---reducing-gc-pressure)* .  \n<small>*Note that it contains some optimizations that are now outdated, but it's still accurate overall.</small>\n\n### 5.2 Caching in L1/2/3\n\nThe second optimization CPUs use is the L1/L2/L3 caches: those are like faster RAMs, but they are also more expensive, so they are much smaller. They contain RAM data, but act as an LRU cache. Data comes in while it's \"hot\" (being worked on), and is written back to the main RAM when new working data needs the space. So the key here is **use as little data as possible to keep your working dataset in the fast caches**. In the following example, we can observe what are the effects of busting each of the successive caches.\n\n<div id=\"benchmark-caches\" class=\"code-blocks\">\n\n```javascript\n// setup:\nconst KB = 1024\nconst MB = 1024 * KB\n\n// These are approximate sizes to fit in those caches. If you don't get the\n// same results on your machine, it might be because your sizes differ.\nconst L1  = 256 * KB\nconst L2  =   5 * MB\nconst L3  =  18 * MB\nconst RAM =  32 * MB\n\n// We'll be accessing the same buffer for all test cases, but we'll\n// only be accessing the first 0 to `L1` entries in the first case,\n// 0 to `L2` in the second, etc.\nconst buffer = new Int8Array(RAM)\nbuffer.fill(42)\n\nconst random = (max) => Math.floor(Math.random() * max)\n```\n\n```javascript\n// 1. L1\nlet r = 0; for (let i = 0; i < 100000; i++) { r += buffer[random(L1)] }\n```\n\n```javascript\n// 2. L2\nlet r = 0; for (let i = 0; i < 100000; i++) { r += buffer[random(L2)] }\n```\n\n```javascript\n// 3. L3\nlet r = 0; for (let i = 0; i < 100000; i++) { r += buffer[random(L3)] }\n```\n\n```javascript\n// 4. RAM\nlet r = 0; for (let i = 0; i < 100000; i++) { r += buffer[random(RAM)] }\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-caches\"\n  results={{\"1. L1\":{\"runTime\":-1000,\"amountOfRounds\":1800,\"percent\":100},\"2. L2\":{\"runTime\":-1000,\"amountOfRounds\":1405,\"percent\":78.06},\"3. L3\":{\"runTime\":-1000,\"amountOfRounds\":1186,\"percent\":65.89},\"4. RAM\":{\"runTime\":-1002,\"amountOfRounds\":528,\"percent\":29.33}}}\n  client:load\n/>\n\n#### What the eff should I do about this?\n\n**Ruthlessly eliminate every single data or memory allocations** that can be eliminated. The smaller your dataset is, the faster your program will run. Memory I/O is the bottleneck for 95% of programs. Another good strategy can be to split your work into chunks, and ensure you work on a small dataset at a time.\n\nFor more details on CPU and memory, [see this link](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf).\n\n<Aside title=\"About immutable data structures\">\n  Immutability is great for clarity and correctness, but in the context of performance, updating an immutable data structure means making a copy of the container, and that's more memory I/O that will flush your caches. You should avoid immutable data structures when possible.\n</Aside>\n\n<Aside title=\"About the { ...spread } operator \">\n  It's very convenient, but every time you use it you create a new object in memory. More memory I/O, slower caches!\n</Aside>\n\n<br />\n<RandomPlant />\n<br />\n\n## 6. Avoid large objects\n\nAs explained in section 2, engines use shapes to optimize objects. However, when the shape grows too large, the engine has no choice but to use a regular hashmap (like a `Map` object). And as we saw in section 5, cache misses decrease performance significantly. Hashmaps are prone to this because their data is usually randomly & evenly distributed over the memory region they occupy. Let's see how it behaves with this map of some users indexed by their ID.\n\n<div id=\"benchmark-large-object\" class=\"code-blocks\">\n\n```javascript\n// setup:\nconst USERS_LENGTH = 1_000\n```\n\n```javascript\n// setup:\nconst byId = {}\nArray.from({ length: USERS_LENGTH }).forEach((_, id) => {\n  byId[id] = { id, name: 'John'}\n})\nlet _ = 0\n```\n\n```javascript\n// 1. [] access\nObject.keys(byId).forEach(id => { _ += byId[id].id })\n```\n\n```javascript\n// 2. direct access\nObject.values(byId).forEach(user => { _ += user.id })\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-large-object\"\n  results={{\"1. [] access\":{\"runTime\":-1000,\"amountOfRounds\":49173,\"percent\":43.18},\"2. direct access\":{\"runTime\":-1000,\"amountOfRounds\":113887,\"percent\":100}}}\n  client:load\n/>\n\nAnd we can also observe how the performance keeps degrading as the object size grows:\n\n<div id=\"benchmark-large-object-larger\" class=\"code-blocks\">\n\n```javascript\n// setup:\nconst USERS_LENGTH = 100_000\n```\n\n<div class=\"hidden\">\n\n```javascript\n// setup:\nconst byId = {}\nArray.from({ length: USERS_LENGTH }).forEach((_, id) => {\n  byId[id] = { id, name: 'John'}\n})\nlet _ = 0\n```\n\n```javascript\n// 1. [] access\nObject.keys(byId).forEach(id => { _ += byId[id].id })\n```\n\n```javascript\n// 2. direct access\nObject.values(byId).forEach(user => { _ += user.id })\n```\n\n</div>\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-large-object-larger\"\n  results={{\"1. [] access\":{\"runTime\":-1005,\"amountOfRounds\":229,\"percent\":20.67},\"2. direct access\":{\"runTime\":-1000,\"amountOfRounds\":1108,\"percent\":100}}}\n  client:load\n/>\n\n#### What the eff should I do about this?\n\nAs demonstrated above, avoid having to frequently index into large objects. Prefer turning the object into an array beforehand. Organizing your data to have the ID on the model can help, as you can use `Object.values()` and not have to refer to the key map to get the ID.\n\n## 7. Use eval\n\nSome javascript patterns are hard to optimize for engines, and by using `eval()` or its derivatives you can make those patterns disappear. In this example, we can observe how using `eval()` avoids the cost of creating an object with a dynamic object key:\n\n\n<div id=\"benchmark-eval\" class=\"code-blocks\">\n\n```javascript\n// setup:\nconst key = 'requestId'\nconst values = Array.from({ length: 100_000 }).fill(42)\n```\n\n```javascript\n// 1. without eval\nfunction createMessages(key, values) {\n  const messages = []\n  for (let i = 0; i < values.length; i++) {\n    messages.push({ [key]: values[i] })\n  }\n  return messages\n}\n\ncreateMessages(key, values)\n```\n\n```javascript\n// 2. with eval\nfunction createMessages(key, values) {\n  const messages = []\n  const createMessage = new Function('value',\n    `return { ${JSON.stringify(key)}: value }`\n  )\n  for (let i = 0; i < values.length; i++) {\n    messages.push(createMessage(values[i]))\n  }\n  return messages\n}\n\ncreateMessages(key, values)\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-eval\"\n  results={{\"1. without eval\":{\"runTime\":-1005,\"amountOfRounds\":233,\"percent\":53.2},\"2. with eval\":{\"runTime\":-1001,\"amountOfRounds\":438,\"percent\":100}}}\n  client:load\n/>\n\nAnother good use-case for `eval` could be to compile a filter predicate function where you discard the branches that you know will never be taken. In general, any function that is going to be run in a very hot loop is a good candidate for this kind of optimization.\n\nObviously the usual warnings about `eval()` apply: don't trust user input, sanitize anything that gets passed into the `eval()`'d code, and don't create any XSS possibility. Also note that some environments don't allow access to `eval()`, such as browser pages with a CSP.\n\n\n\n## 8. Use strings, carefully\n\nWe've already seen above how strings are more expensive than they appear. Well I have kind of a good news/bad news situation here, which I'll announce in the only logical order (bad first, good second): strings are more complex than they appear, but they can also be quite efficient used well.\n\nString operations are a core part of JavaScript due to its context. To optimize string-heavy code, engines had to be creative. And by that I mean, they had to represent the `String` object with multiple string representation in C++, depending on the use case. There are two general cases you should worry about, because they hold true for V8 (the most common engine by far), and generally also in other engines.\n\nFirst, strings concatenated with `+` don't create a copy of the two input strings. The operation creates a pointer to each substring. If it was in typescript, it would be something like this:\n\n```typescript\nclass String {\n  abstract value(): char[] {}\n}\n\nclass BytesString {\n  constructor(bytes: char[]) {\n    this.bytes = bytes\n  }\n  value() {\n    return this.bytes\n  }\n}\n\nclass ConcatenatedString {\n  constructor(left: String, right: String) {\n    this.left = left\n    this.right = right\n  }\n  value() {\n    return [...this.left.value(), ...this.right.value()]\n  }\n}\n\nfunction concat(left, right) {\n  return new ConcatenatedString(left, right)\n}\n\nconst first = new BytesString(['H', 'e', 'l', 'l', 'o', ' '])\nconst second = new BytesString(['w', 'o', 'r', 'l', 'd'])\n\n// See ma, no array copies!\nconst message = concat(first, second)\n```\n\nSecond, string slices also don't need to create copies: they can simply point to a range in another string. To continue with the example above:\n\n```typescript\nclass SlicedString {\n  constructor(source: String, start: number, end: number) {\n    this.source = source\n    this.start = start\n    this.end = end\n  }\n  value() {\n    return this.source.value().slice(this.start, this.end)\n  }\n}\n\nfunction substring(source, start, end) {\n  return new SlicedString(source, start, end)\n}\n\n// This represents \"He\", but it still contains no array copies.\n// It's a SlicedString to a ConcatenatedString to two BytesString\nconst firstTwoLetters = substring(message, 0, 2)\n```\n\nBut here's the issue: once you need to start mutating those bytes, that's the moment you start paying copy costs. Let's say we go back to our `String` class and try to add a `.trimEnd` method:\n\n```typescript\nclass String {\n  abstract value(): char[] {}\n\n  trimEnd() {\n    // `.value()` here might be calling\n    // our Sliced->Concatenated->2*Bytes string!\n    const bytes = this.value()\n\n    const result = bytes.slice()\n    while (result[result.length - 1] === ' ')\n      result.pop()\n    return new BytesString(result)\n  }\n}\n```\n\nSo let's jump to an example where we compare using operations that use mutation versus only using concatenation:\n\n<div id=\"benchmark-strings\" class=\"code-blocks\">\n\n```javascript\n// setup:\nconst classNames = ['primary', 'selected', 'active', 'medium']\n```\n\n```javascript\n// 1. mutation\nconst result =\n  classNames\n    .map(c => `button--${c}`)\n    .join(' ')\n```\n\n```javascript\n// 2. concatenation\nconst result =\n  classNames\n    .map(c => 'button--' + c)\n    .reduce((acc, c) => acc + ' ' + c, '')\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-strings\"\n  iterations={5000}\n  results={{\"1. mutation\":{\"runTime\":-1000,\"amountOfRounds\":1808,\"percent\":37.43},\"2. concatenation\":{\"runTime\":-1000,\"amountOfRounds\":4830,\"percent\":100}}}\n  client:load\n/>\n\n#### What the eff should I do about this?\n\nIn general, try to **avoid mutation for as long as possible**. This includes methods such as `.trim()`, `.replace()`, etc. Consider how you can avoid those methods. In some engines, string templates can also be slower than `+`. In V8 at the moment it's the case, but might not be in the future so as always, benchmark.\n\nA note on `SlicedString` above, you should note that if a small substring to a very large string is alive in memory, <mark title=\"it will, in V8\">it might</mark> prevent the garbage collector from collecting the large string! If you're processing large texts and extracting small strings from it, you might be leaking large amounts of memory.\n\n```javascript\nconst large = Array.from({ length: 10_000 }).map(() => 'string').join('')\nconst small = large.slice(0, 50)\n//    ^ will keep `large` alive\n```\n\nThe solution here is to use mutation methods to our advantage. If we use one of them on `small`, it will force a copy, and the old pointer to `large` will be lost:\n\n```javascript\n// replace a token that doesn't exist\nconst small = small.replace('#'.repeat(small.length + 1), '')\n```\n\nFor more details, see [string.h on V8](https://github.com/v8/v8/blob/main/src/objects/string.h) or [JSString.h on JavaScriptCore](https://github.com/WebKit/WebKit/blob/main/Source/JavaScriptCore/runtime/JSString.h).\n\n<Aside title=\"On strings complexity\">\n  I have skimmed very quickly over things, but there are a lot of implementation details that add complexity to strings. There are often minimum lengths for each of those string representations. For example a concatenated string might not be used for very small strings. Or sometimes there are limits, for example avoiding pointing to a substring of a substring. Reading the C++ files linked above gives a good overview of the implementation details, even if just reading the comments.\n</Aside>\n\n\n## 9. Use specialization\n\nOne important concept in performance optimization is *specialization*: adapting your logic to fit in the constraints of your particular use-case. This usually means figuring out what conditions are *likely* to be true for your case, and coding for those conditions.\n\nLet's say we are a merchant that sometimes needs to add tags to their product list. We know from experience that our tags are usually empty. Knowing that information, we can specialize our function for that case:\n\n<div id=\"benchmark-specialization\" class=\"code-blocks\">\n\n```javascript\n// setup:\nconst descriptions = ['apples', 'oranges', 'bananas', 'seven']\nconst someTags = {\n  apples: '::promotion::',\n}\nconst noTags = {}\n\n// Turn the products into a string, with their tags if applicable\nfunction productsToString(description, tags) {\n  let result = ''\n  description.forEach(product => {\n    result += product\n    if (tags[product]) result += tags[product]\n    result += ', '\n  })\n  return result\n}\n\n// Specialize it now\nfunction productsToStringSpecialized(description, tags) {\n  // We know that `tags` is likely to be empty, so we check\n  // once ahead of time, and then we can remove the `if` check\n  // from the inner loop\n  if (isEmpty(tags)) {\n    let result = ''\n    description.forEach(product => {\n      result += product + ', '\n    })\n    return result\n  } else {\n    let result = ''\n    description.forEach(product => {\n      result += product\n      if (tags[product]) result += tags[product]\n      result += ', '\n    })\n    return result\n  }\n}\nfunction isEmpty(o) { for (let _ in o) { return false } return true }\n\n```\n\n```javascript\n// 1. not specialized\nfor (let i = 0; i < 100; i++) {\n  productsToString(descriptions, someTags)\n  productsToString(descriptions, noTags)\n  productsToString(descriptions, noTags)\n  productsToString(descriptions, noTags)\n  productsToString(descriptions, noTags)\n}\n```\n\n```javascript\n// 2. specialized\nfor (let i = 0; i < 100; i++) {\n  productsToStringSpecialized(descriptions, someTags)\n  productsToStringSpecialized(descriptions, noTags)\n  productsToStringSpecialized(descriptions, noTags)\n  productsToStringSpecialized(descriptions, noTags)\n  productsToStringSpecialized(descriptions, noTags)\n}\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-specialization\"\n  iterations={5000}\n  results={{\"1. not specialized\":{\"runTime\":-1118,\"amountOfRounds\":6,\"percent\":85.71},\"2. specialized\":{\"runTime\":-1062,\"amountOfRounds\":7,\"percent\":100}}}\n  client:load\n/>\n\nThis sort of optimization can give you moderate improvements, but those will add up. They are a nice addition to more crucial optimizations, like shapes and memory I/O. But note that specialization can turn against you if your conditions change, so be careful when applying this one.\n\n<Aside title=\"Branch prediction and branchless code\">\n  Removing branches from your code can be incredibly efficient for performance. For more details on what a branch predictor is, read the classic Stack Overflow answer [Why is processing a sorted array faster](https://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-processing-an-unsorted-array).\n</Aside>\n\n<br />\n\n## 10. Data structures\n\nI won't go in details about data structures as they would require their own post. But be aware that using the incorrect data structures for your use-case can have a **bigger impact than any of the optimizations above**. I would suggest you to be familiar with the native ones like `Map` and `Set`, and to learn about linked lists, priority queues, trees (RB and B+) and tries.\n\nBut for a quick example, let's compare how `Array.includes` does against `Set.has` for a small list:\n\n<div id=\"benchmark-data-structures\" class=\"code-blocks\">\n\n```javascript\n// setup:\nconst userIds = Array.from({ length: 1_000 }).map((_, i) => i)\nconst adminIdsArray = userIds.slice(0, 10)\nconst adminIdsSet = new Set(adminIdsArray)\n```\n\n```javascript\n// 1. Array\nlet _ = 0\nfor (let i = 0; i < userIds.length; i++) {\n  if (adminIdsArray.includes(userIds[i])) { _ += 1 }\n}\n```\n\n```javascript\n// 2. Set\nlet _ = 0\nfor (let i = 0; i < userIds.length; i++) {\n  if (adminIdsSet.has(userIds[i])) { _ += 1 }\n}\n```\n\n</div>\n\n<Benchmark\n  selector=\"#benchmark-data-structures\"\n  results={{\"1. Array\":{\"runTime\":-1000,\"amountOfRounds\":89836,\"percent\":34.27},\"2. Set\":{\"runTime\":-1000,\"amountOfRounds\":262116,\"percent\":100}}}\n  client:load\n/>\n\nAs you can see, the data structure choice makes a very impactful difference.\n\nAs a real-world example, I had a case where we were able to [reduce the runtime of a function from 5 seconds to 22 milliseconds](https://github.com/mui/mui-x/pull/9200) by switching out an array with a linked list.\n\n<RandomPlant className='mt-8 mb-16' />\n\n## 11. Benchmarking\n\nI've left this section for the end for one reason: I needed to establish credibility with the fun sections above. Now that I (hopefully) have it, let me tell you that benchmarking is the most important part of optimization. Not only is it the most important, but it's also *hard*. Even after 20 years of experience, I still sometimes create benchmarks that are flawed, or use the profiling tools incorrectly. So whatever you do, please **put the most effort into benchmarking correctly**.\n\n### 11.0 Start with the top\n\nYour priority should always be to optimize the function/section of code that makes up the biggest part of your runtime. If you spend time optimizing anything else than the top, you are wasting time.\n\n### 11.1 Avoid micro-benchmarks\n\nRun your code in production mode and base your optimizations on those observations. JS engines are very complex, and will often behave differently in micro-benchmarks than in real-world scenarios. For example, take this micro-benchmark:\n\n```javascript\nconst a = { type: 'div', count: 5, }\nconst b = { type: 'span', count: 10 }\n\nfunction typeEquals(a, b) {\n  return a.type === b.type\n}\n\nfor (let i = 0; i < 100_000; i++) {\n  typeEquals(a, b)\n}\n```\n\nIf you've payed attention sooner, you will realize that the engine will specialize the function for the shape `{ type: string, count: number }`. But does that hold true in your real-world use-case? Are `a` and `b` always of that shape, or will you receive any kind of shape? If you receive many shapes in production, this function will behave differently then.\n\n### 11.2 Doubt your results\n\nIf you've just optimized a function and it now runs 100x faster, doubt it. Try to disprove your results, try it in production mode, throw stuff at it. Similarly, doubt also your tools. The mere fact of observing a benchmark with devtools can modify its behavior.\n\n### 11.3 Pick your target\n\nDifferent engines will optimize certain patterns better or worse than others. You should benchmark for the engine(s) that are relevant to you, and prioritize which one is more important. [Here's a real-world example](https://github.com/babel/babel/pull/16357) in Babel where improving V8 means decreasing JSC's performance.\n\n## 12. Profiling & tools\n\nVarious remarks about profiling and devtools.\n\n### 12.1 Browser gotchas\n\nIf you're profiling in the browser, make sure you use a clean and empty browser profile. I even use a separate browser for this. If you're profiling and you have browser extensions enabled, they can mess up the measurements. React devtools in particular will substantially affect results, rendering code may appear slower than it appears ~in the mirror~ to your users.\n\n### 12.2 Sample vs structural profiling\n\nBrowser profiling tools are sample-based profilers, which take a sample of your stack at regular intervals. This had a big disadvantage: very small but very frequent functions might be called between those samples, and might be very much underreported in the stack charts you'll get. Use Firefox devtools with a custom sample interval or Chrome devtools with CPU throttling to mitigate this issue.\n\n### 12.3 The tools of the trade\n\nBeyond the regular browser devtools, it may help to be aware of these options:\n\n- Chrome devtools have quite a few experimental flags that can help you figure out why things are slow. The style invalidation tracker is invaluable when you need to debug style/layout recalculations in the browser.  \n  https://github.com/iamakulov/devtools-perf-features\n\n- The deoptexplorer-vscode extension allows you to load V8/chromium log files to understand when your code is triggering deoptimizations, such as when you pass different shapes to a function. You don't need the extension to read log files, but it makes the experience much more pleasant.  \n https://github.com/microsoft/deoptexplorer-vscode\n\n- You can always compile the debug shell for each JS engine to understand more in details how it works. This allows you to run `perf` and other low-level tools, and also to inspect the bytecode and machine code generated by each engine.  \n [Example for V8](https://mrale.ph/blog/2018/02/03/maybe-you-dont-need-rust-to-speed-up-your-js.html#getting-the-code) | [Example for JSC](https://zon8.re/posts/jsc-internals-part1-tracing-js-source-to-bytecode/) | <span class=\"text-neutral-500\">Example for SpiderMonkey (missing)</span>\n\n\n## Final notes\n\nHope you learned some useful tricks. If you have any comments, corrections or questions, email in the footer. I'm always happy to receive feedback or questions from readers.\n\n<RandomPlant className='mt-8 mb-32' />\n\n<div className='text-neutral-500'>\n\nIf you've made it this far, I invite you to view [The Castle](/castle).\n\n</div>","src/content/posts/optimizing-javascript.mdx","28143664a6af6dcb","optimizing-javascript.mdx","react-functional-components",{id:69,data:71,body:76,filePath:77,digest:78,legacyId:79,deferredRender:19},{title:72,description:73,pubDate:74,sidebar:75,rating:52},"The problem with functional components","romgrk ranting about the deprecation of class components","Oct 6 2023",{display:18},"import RandomPlant from '../../components/client/RandomPlant/RandomPlant.tsx'\nimport { ExampleStack, ExamplePerformance } from '../../components/client/posts/react-functional-components.tsx'\n\n\nOver the last years, with the introduction of functional components and hooks, React has gradually been phasing out class components. Although I appreciate the introduction of functional components for their simplicity & conciseness, I think that <mark title=\"read the addendum\">removing class components</mark> was the wrong decision.\n\nBefore going further, let's establish one thing: closures and classes are two ways to look at the same thing: it's a pointer to a set of fields & methods. For classes, we call this pointer `this`. For closures, we call this pointer the closure context. In theory, both constructs are interchangeable, so we should be free to use whichever one is more adapted to the situation at hand. In the following sections, I'll be arguing that *in some cases*, being able to use classes is more beneficial than using closures. Essentially, simple components are better written as closures, whereas complex components with more state are better written as classes.\n\nLet's look at an example. Below we have equivalent drafts for a `Selector` component in functional and class versions.\n\n<div class=\"code-blocks-row mb-1\">\n\n```tsx\nclass Selector extends Component {\n  // fields\n  id = cuid()\n  ref = createRef()\n  position = { x: 0, y: 0}\n\n  // state\n  state = {\n    open: false,\n    loading: false,\n    disabled: false,\n    selectedIndex: -1,\n  }\n\n  // methods\n  open() {}\n  close() {}\n  select(index) {}\n  focus() {}\n\n  // result\n  render() {\n    return (\n      /* ... */\n    )\n  }\n}\n```\n\n```tsx\nfunction Selector(props) {\n  // fields\n  const id = useRef(cuid())\n  const ref = useRef()\n  const position = useRef({ x: 0, y: 0 })\n\n  // state\n  const [open, setOpen] =\n    useState(false)\n  const [loading, setLoading] =\n    useState(false)\n  const [disabled, setDisabled] =\n    useState(false)\n  const [selectedIndex, setSelectedIndex] =\n    useState(-1)\n\n  // methods\n  const open = useCallback(() => {}, [])\n  const close = useCallback(() => {}, [])\n  const select = useCallback((index) => {}, [])\n  const focus = useCallback(() => {}, [])\n\n  // result\n  return (\n    /* ... */\n  )\n}\n```\n\n</div>\n\n### Issue #1: Readability is worst\n\nWhereas the class version uses idiomatic standard javascript syntax, the functional one replicates the same structure by mashing every field, state and method into one big function, where method declaration, field initialization and UI rendering intermingle into one ungodly mess. Class syntax provides me language-backed guarantees that when I look at a method, be it `a()`, `b()` or `render()`, no control flow outside of the function can affect the control flow inside the function and no variable declared inside the function can be used anywhere else. There is a guaranteed encapsulation. I know where the `a` part is, I know where the `render` part is.\n\nFor the functional version, all the state and temporary variables are stored in the same lexical context. If you're lucky, the component follows the same structure than it would if it was a class, and temporary variables are declared next to the function where they're used. If you're not, they're interspersed a bit everywhere. Some rendering done here. Oh and some more done after we've defined this new function. Oh. ho. o. !.\n\nAnd you need to wrap everything in `useState`, `useMemo` and `useCallback`. Every field. Every method. Everywhere.\n\nAnd then comes the problem of passing part of the internal state of the component around. Say you want to decouple part of the processing to functions in another file. How do you pass the internal state of the component around? For class components, that's trivial: pass `this`. For functional components, `this` is instead the closure context which is... a data structure accessible only to the JS engine :|\nSo what do you do? Either create bag-of-data objects from the state (essentially duplicating the closure context in memory): `externalFn({ open, loading, select })`. Or mash more functions into the big function.\n\nAnd the worst? I lied above. To be 100% equivalent you'd need to wrap with `forwardRef` and expose the methods with `useImperativeHandle`:\n\n```tsx\nconst Selector = forwardRef(function Selector(props, ref) {\n  /* same as above: fields, state, methods */\n\n  useImperativeHandle(ref, () => ({\n    open,\n    close,\n    select,\n    focus,\n  }), [])\n\n  /* same as above: result */\n})\n\n```\n\n### Issue #2: DX is worst\n\n<div class=\"if-firefox text-warning\">NOTE: You appear to be on firefox. This section doesn't apply for your browser.</div>\n\nLet's say you're debugging some bug that happens in response to an event. The handler calls down some function. Which calls another. Few levels more. You know where the bug happens. You've set your `debugger` breakpoint, deep down in the stack. What does your call stack look like?\n\n<ExampleStack client:load />\n\nYup. `useCallback` prevents the function from being assigned directly to a variable, so it's anonymous and the engine can't guess it's name*. This is what you get in the devtools everytime you look at your call stack. Wanna figure what calls resulted in this bug? Go look at every entry of the stack manually.\n\n*\\* SpiderMonkey is the only exception :O V8 and JSC can't.*\n\n### Issue #3: Performance is worst\n\nThis one is obvious. Every time your component re-renders, it needs to make:\n\n```tsx\n/* +1 allocation: return array */\nconst [state, setState] = useState(false)\n\n/* +2 allocations: inline closure, dependency array */\nconst method = useCallback(() => {}, [])\n\n/* +2 allocations: inline closure, dependency array */\nconst derived = useMemo(() => {}, [props.data])\n```\n\nIn addition to those, which are \"React's fault\", there are also just more allocations by default because there is no differentiation between initialization (like you know, a `constructor()`) and rendering, so people keep doing allocations like this one:\n\n```tsx\n/* +1 allocation: inline object */\nconst position = useRef({ x: 0, y: 0})\n```\n\nAnyway let's test it with my poor man's implementation of react hooks. It's accurate in terms of performance overhead, the react hooks implementation also uses a linked list (click Show Context to see the implementation).\n\n<ExamplePerformance client:load />\n\nAlright, I'll admit that the benchmark here is a bit of a stretch, and that the impact for the large majority of applications is irrelevant. But in some cases, it does. You're running this on a <mark title=\"I'm guessing I have no idea\">fast</mark> device, what would be the result on a low-end device? What happens when you do get to those stretching use-cases? Then you don't have a choice but to use the less performant functional components.\n\nOf course if you have a (real) functional component that is just a pure function `Component :: props -> UI` with no state & hooks, then for sure the functional one is faster. Functional components do have a place, they're excellent for simple use-cases. But there's more than just simple use-cases in life.\n\n## Final notes\n\nI wish React would keep class components. But it seems like the maintainers in the last years are forcefully making it impossible to shoot yourself in the foot and make it so there is One Right Way to do things (see the removal of `componentWillReceiveProps`). Which makes sense, from their point of view. They're maintaining a library with a huge reach. They get all sort of reports from people with very varying levels of skill. So the biggest problem from their perspective is to ensure that most people are using the library correctly. And that means making things as simple as possible so those with less experience can use it correctly. They baby-proof it. The thing is, by doing so they make it harder to solve hard problems.\n\n<br/>\n<RandomPlant />\n<br/>\n<br/>\n\n<hr/>\n\n### Addendum: *\"but they're not removing class components\"*\n\nAny code written with hooks cannot be used from class components, therefore it's not possible to write new code with classes because the default style is functional components. The maintainers have also stopped making things usable. It's not possible to use more than one context in a class component (at least in an ergonomic way). And all the new improvements (e.g. `useEffect`: running something when a specific set of state/props changes) are also not included in the class components API. So yes, they're removing them.","src/content/posts/react-functional-components.mdx","5b10db9d292b915b","react-functional-components.mdx","computers-from-scratch",{id:80,data:82,body:85,filePath:86,digest:87,legacyId:88,deferredRender:19},{title:83,description:28,pubDate:84,draft:19},"Computers from scratch","Sep 21 2023","import { DemoTransistor } from '../../components/client/posts/computers-from-scratch/transistor.tsx'\nimport { DemoNotGate } from '../../components/client/posts/computers-from-scratch/logicGates.tsx'\n\nIn this series, I'll be building a virtual real 16-bit computer from scratch, starting from the transistor and moving up the stack, going through logic gates, registers, ALU, RAM, caches (L1, L2, etc), MMU, assembly, basic compiled language and finally basic interpreted language. At the end of this, hopefully you should be \"fullstack\" for real!\n\nNow, when I say \"virtual real\" computer, I understand that can be confusing. What I mean is that we'll be using a simulation of transistors & electrical circuits, and we'll build our computer inside this simulation. This will also allow us (well you) to play with the actual circuits. All graphes here are **🌈 fully interactive 🌈**.\n\n## Transistors\n\nThe basic component of computers is the transistor. Everything is built from them. A bit like emptiness underlies all form. Transistors are pretty simple: if there is current going to the `control` pin, then the current can flow between `input` and `output`. You can toggle the `ON/OFF` power supply to verify that by yourself. \n\n<DemoTransistor client:load />\n\nThis simple mechanism is what underlies all of computing, and from which we can build all the more complex behavior that we're about to see. Speaking of which, let's jump to the second level above transistors: logic gates.\n\n## Logic gates\n\nBuilding logic gates with our transistors is about acquiring the building blocks required to perform boolean algebra, because boolean algebra is all we need to build the rest. The graphs in this section will be shown in two versions: the electrical one and the logical one. The electrical one shows how one such circuit can be built in real life, while the logical one shows the pure logic gate, unencumbered by electricity. This is the last step where we'll worry about electricity, all the subsequent ones will be based on the logic gates only.\n\n<DemoNotGate client:load />","src/content/posts/computers-from-scratch.mdx","9528bbad2def0a39","computers-from-scratch.mdx","reactivity-is-easy",{id:89,data:91,body:95,filePath:96,digest:97,legacyId:98,deferredRender:19},{title:92,description:28,pubDate:93,sidebar:94},"Reactivity is easy","Jun 7 2025",{display:18},"import Aside from '../../components/Aside.astro';\nimport RandomPlant from '../../components/client/RandomPlant/RandomPlant.tsx';\nimport { Naive, Optimized } from './reactivity-is-easy/index.tsx';\nimport './reactivity-is-easy/index.css';\n\nReactivity as a concept seems to be still misunderstood in the React ecosystem, and I wanted to provide a write-up of how we solved that problem in the MUI X Data Grid. I think fine-grained selector-based reactivity is possible in React in **less than 35 lines**, and I will give you a copy-pastable example of that by the end of this post (and a package for the lazy ones).\n\nI think it's important to be able to reduce a problem to its most minimalist solution, because it highlights more clearly what the problem is about. Simplicity is also one of the highest aims we should have for our code, because from simple code emerges easy maintainability and easy performance optimization. Understanding the most minimalist solution to a problem also allows you to build up from it, rather than being handed down a pre-built solution that you don't understand.\n\n## The problem at hand\n\nTo replicate the problem we had in the Data Grid, here is a simple runnable example. It's a `Grid` with `Cell` components inside it. It stores the currently focused cell in a state at the root of the grid, and each cell can update the state when it gets focused:\n\n```jsx\nconst Context = createContext();\n\nfunction Grid() {\n  const [focus, setFocus] = useState(0);\n  const context = useMemo(() => ({ focus, setFocus }), [focus]);\n\n  return (\n    <Context.Provider value={context}>\n      {Array.from({ length: 50 }).map((_, i) => (\n        <Cell index={i} />\n      ))}\n    </Context.Provider>\n  );\n}\n\nfunction Cell({ index }) {\n  const context = useContext(Context);\n  const focus = context.focus === index;\n  return (\n    <button\n      onClick={() => context.setFocus(index)}\n      className={clsx({ focus })}\n    >\n      {index}\n    </button>\n  );\n};\n```\n\nWith a bit of styling and some instrumentation to flash-highlight cells when they re-render, here is our grid. You'll notice that each time you click on a cell, **all the cells re-render** :/\n\n<Naive.Grid client:load />\n<br />\n\nBecause the root `Context` value needs to change to store the new `focus` value, then every cell that uses that context also needs to re-render to get to use it. This is an unsatisfying state of things considering that we might have a lot of cells, and each cell may be expensive to render.\n\n## A solution\n\nI promised you a solution in less than 35 lines of code, here it is. A store is essentially just a ref object that triggers callbacks when it changes. And those callbacks just need to trigger a targetted re-render, which we can do simply by calling a `setState` hook function in each component.\n\n```tsx showLineNumbers\ntype Listener<S> = (s: S) => void;\n\nclass Store<State> {\n  public state: State;\n  private listeners: Set<Listener<State>>;\n\n  constructor(state: State) {\n    this.state = state;\n    this.listeners = new Set();\n  }\n\n  public subscribe = (fn: Listener<State>) => {\n    this.listeners.add(fn);\n    return () => { this.listeners.delete(fn); };\n  };\n\n  public update = (newState: State) => {\n    this.state = newState;\n    this.listeners.forEach((l) => l(newState));\n  };\n}\n\nfunction useSelector(store, selector, ...args) {\n  const [value, setValue] =\n    useState(() => selector(store.state, ...args));\n\n  useEffect(() =>\n    store.subscribe((state) =>\n      setValue(selector(state, ...args)))\n  , []);\n\n  return value;\n}\n```\n\nTo use it, all we have to do is place our `Store` instance in a context, and then every component can subscribe to store updates via `useSelector`. Because the selectors select the precise slice of state that a component is interested in, it will not re-render as long as that slice doesn't change.\n\n```tsx\nconst Context = createContext();\n\nexport function Grid() {\n  const [store] = useState(() => new Store({ focus: 0 }));\n\n  return (\n    <Context.Provider value={store}>\n      {Array.from({ length: 50 }).map((_, i) => (\n        <Cell index={i} />\n      ))}\n    </Context.Provider>\n  );\n}\n\nconst selectors = {\n  isFocus: (state, index) => state.focus === index,\n};\n\nfunction Cell({ index }) {\n  const store = useContext(Context);\n  const focus = useSelector(store, selectors.isFocus, index);\n\n  return (\n    <button\n      ref={ref}\n      onClick={() => store.update({ ...store.state, focus: index })}\n      className={clsx({ focus })}\n    >\n      {index}\n    </button>\n  );\n};\n```\n\nAnd finally, here is our updated example. You'll notice that when you click a cell, only the two cells for which the focus changed re-render. All the other ones never have to ever update.\n\n<Optimized.Grid client:load />\n<br />\n\nIf you've noticed, in this example we didn't even have to use `React.memo()` to avoid re-renders! The reason it's not useful is because the store updates target the `useState` hook inside each cell, so even though the store state changes, the outer `Grid` never needs to update, therefore it never re-renders its children either. Fine-grained reactivity is so simple, precise, and enjoyable. In practice, once the root component starts using state, you'll want to have `React.memo()` though.\n\n<Aside title=\"When does a component rerender?\">\nAs a reminder, components rerender when either of these is true:\n\n1. Their parent re-rendered\n2. Either `useState`, `useReducer` or `useContext` changed.\n\nAnd `React.memo()` is an escape hatch for the 1st point.\n</Aside>\n\n<RandomPlant className=\"mt-8 mb-24\" />\n\n## Building up\n\nAlright, now that we've established the most minimalist solution here are a few more things to consider.\n\n### React edge-cases\n\nThe `useSelector` implementation I provided above is nice to understand the concept, but you might run into edge-cases. Since React introduced a new async rendering model, state tearing can happen (analogous to [screen tearing](https://en.wikipedia.org/wiki/Screen_tearing)). To handle that, you need to use `use-sync-external-store`, or if you're using React 18 and above, just `React.useSyncExternalStore`. The package provides a shim for older versions though, and there's barely any bundle-size to it so I recommend it.\n\n\n```tsx\nimport {\n  useSyncExternalStoreWithSelector\n} from 'use-sync-external-store/with-selector';\n\nfunction useSelector(store, selector, ...args) {\n  return useSyncExternalStoreWithSelector(\n    store.subscribe,\n    store.getSnapshot,\n    store.getSnapshot,\n    (state) => selector(state, ...args),\n  );\n}\n```\n\n### A more ergonomic store?\n\nYou might have also noticed that updating the store is a bit of a mouthful. Having to write `store.update({ ...store.state, focus: 42 })` is tedious, it will be even more so once you have a deeper state object. So you might want to add utility methods on the store to be able to write more simply `store.set('focus', 42)`:\n\n```tsx showLineNumbers {4-6}\nclass Store<State> {\n  /* ... */\n\n  public set<K extends keyof State>(key: K, value: State[K]) => {\n    this.update({ ...this.state, [key]: value });\n  }\n\n  /* ... */\n}\n```\n\nHere is a simple `.set()` implementation, but you can build you own by using your favourite utility-belt library to set paths directly, e.g. `lodash.set(state, 'a[0].b.c', 42)`.\n\n### Deriving state & computed values\n\nThe minimalist selectors above are just plain functions, and that's more than enough to understand the concept. However for a production use-case like we had in the Data Grid, being able to compute values derived from the state is essential. To solve that problem, we introduced **memoized selectors**. To avoid re-inventing the wheel, we just used redux's `reselect` implementation of `createSelectorMemoized` ([docs link](https://reselect.js.org/api/createselector/)):\n\n```tsx\nimport {\n  createSelector as createSelectorMemoized\n} from 'reselect'\n\n// Imagine a datagrid, with a set of rows and a \"sortBy\"\n// key to sort those rows.\nconst store = new Store({ rows: [\n  { id: 1, name: 'John' },\n  { id: 2, name: 'Alice' },\n  { id: 3, name: 'Bob' },\n], sortBy: 'id' })\n\nconst rowsSelector   = state => state.rows\nconst sortBySelector = state => state.sortBy\n\nconst sortedRowsSelector = createSelectorMemoized(\n  // The selector uses these 2 other selectors as its inputs.\n  rowsSelector,\n  sortBySelector,\n\n  // Instead of receiving the `state`, it receives the return values\n  // of the selectors above, and it outputs a sorted rows array.\n  (rows, sortBy) => {\n    return rows.toSorted((a, b) => compare(a[sortBy], b[sortBy]))\n  }\n)\n```\n\nAnd then using that selector is as simple as using any other selector:\n\n```tsx\nfunction Component() {\n  const store = useContext(Context)\n  const sortedRows = useSelector(store, sortedRowsSelector)\n  /* ... */\n}\n```\n\nAnd `sortedRows` never gets computed more than it needs to 🪄.\n\nIn practice we write all our selectors with `createSelectorMemoized` and an equivalent non-memoized `createSelector`, which lets us write them in a consistent way as well as add some instrumentation over them.\n\n```tsx\nconst rows =  createSelector((state: State) => state.rows)\nconst sortBy = createSelector((state: State) => state.sortBy)\nconst sortedRows = createSelectorMemoized(\n  rows, sortBy,\n  (rows, sortBy) =>\n    rows.toSorted((a, b) => compare(a[sortBy], b[sortBy]))\n)\n```\n\nThis syntax could also allow us in the future (with some magic) to possibly switch to an event-based reactivity model, while using a selector-based syntax 🙈. The concept is based on a automatic subscription model similar to SolidJS signals. That's for another post though.\n\n<Aside title=\"When to use memoization?\">\nThe rule for using memoization in selectors is:\n\n1. The result is a derived non-primitive value.\n2. The selector is computationally expensive.\n\nDon't under-use nor over-use it, or you'll suffer consequences.\n</Aside>\n\n\n## I want a package\n\nYou probably want a package and I don't blame you, I'm also very lazy.\n\nI've published this code including the building-up section as a package on NPM as [store-x-selector](https://www.npmjs.com/package/store-x-selector).\n\nIt contains a few more performance optimizations to make selector with arguments as cost-less as possible (for example, it doesn't use `...args` to avoid an array allocation), and it also contains accurate typings for all of this.\n\n## Final notes\n\nIf you have any comments, corrections or questions, email in the footer. I'm always happy to receive feedback or questions from readers.\n\n<RandomPlant className=\"mt-8 mb-32\" />\n\n<div className='text-neutral-500'>\n\nIf you've made it this far, I invite you to view [The Castle](/castle).\n\n</div>","src/content/posts/reactivity-is-easy.mdx","3ea577f07063e2d8","reactivity-is-easy.mdx","styled-components-bad",{id:99,data:101,body:106,filePath:107,digest:108,legacyId:109,deferredRender:19},{title:102,description:103,pubDate:104,rating:105},"Why styled components are bad","romgrk ranting about styled components","Nov 28 2023",3,"import RandomPlant from '../../components/client/RandomPlant/RandomPlant.tsx'\n\nLet's say you open a file in an unknown project, or a project you worked on a long time ago (which is basically the same thing), and jump directly to the render method of a component that you need to work on. This is what you see:\n\n```jsx\nfunction Component() {\n  return (\n    <Glorb>\n      <Obul>\n        <Zirk />\n      </Obul>\n    </Glorb>\n  )\n}\n```\n\nNow, assume this project uses styled-components. What are `Glorb`, `Obul` and `Zirk`:\n - Styled components that only add some styling?\n - Real components that encapsulate logic & behavior?\n\n*There is no way to know!* It could very well be any of these possibilities:\n\n```tsx\n/* just some cute styling :) */\nexport const Glorb = styled.div`\n  border: 1px solid salmon;\n`\n```\n\n```tsx\nexport function Glorb({ children }) {\n  /* lots of logic */\n  useEffect(() => { /* ... */ }, [])\n  /* and funky stuff */\n  return (\n    React.Children.map(children, child => /* ... */)\n  )\n}\n```\n\nThe only solution left at this point for you is to jump to the definition of each of those components to try to make sense if any of them does something funky, before you work further on whatever you need to fix. In other words, you have no guarantees about what's going on in that first snippet. You can't make assumptions about what those components are.\n\nBesides, the JSX tag name is not a natural place to encode styling. The tag name for elements has always been for the logic & behavior it encodes, be it a `div`, `button`, `input` or even custom ones like `Combobox` or `DatePicker`. You're going to be able to tell from the name right away what behavior it encapsulates. So what is the best place to encode styling, you ask me? The same place that we've been using for decades: `class(Name)` and `style`.\n\nNow imagine again you come in an unknown project and you look at this:\n\n```tsx\nfunction Component() {\n  return (\n    <div className={glorb}>\n      <div className={obul}>\n        <div className={zirk} />\n      </div>\n    </div>\n  )\n}\n```\n\nSure, it's very slightly more verbose. On the other hand you know for a fact that all there is here are divs and nothing funky is going to happen. `glorb`, `obul` and `zirk` are classnames and you don't need to jump anywhere to figure that out. If you later need to add styling via props, you're already set up to `clsx` everything:\n\n```tsx\nfunction Component({ className }) {\n  return (\n    <div className={clsx(glorb, className)}>\n      { /* etc */ }\n    </div>\n  )\n}\n/*\n * Note how everything style is defined at a single point.\n * No need to glance at the tag to figure out if it adds styling as well,\n * you just know instantly it's all in the className.\n * Simple, predictable, consistent.\n */\n```\n\n### Reusability\n\nThere's also the fact that styles gone into a styled components are not reusable. Say you want to reuse and combine the styles of `Glorb` and `Obul`. How you do?\n\n```tsx\nconst Glorb = styled.div`color: red;`\nconst Obul = styled.div`background-color: yellow;`\nfunction Component() {\n  return (\n    <Glorb,Obul /> // <!--- ??? :')\n  )\n}\n```\n\nIf you wanted to make it work, you'd need to both define the styled components *and* expose the styles CSS string for others to re-use.\n\nWhereas if you have classNames `glorb` and `obul`, you again simply `clsx` and you're done. You can expose them & export them, and it works anywhere. Classes are native to the ecosystem and everyone understands them.\n\n```tsx\nconst glorb = css`color: red;`\nconst obul = css`background-color: yellow;`\n\nfunction Component() {\n  return (\n    <div className={clsx(glorb, obul)} />\n  )\n}\n```\n\n### Performance\n\nIf you're using styled components, each of them is roughly going to be doing this:\n\n```tsx\nfunction StyledButton({ className, ...rest }) {\n  return (\n    <button className={clsx(className, autogeneratedStyleClassName)} {...rest} />\n  )\n}\n```\n\nIf you're using styled components to style everything, then you're roughly doubling the cost of rendering your whole app. Admittedly your logical components will be doing more work so it's going to be a factor between 1 and 2, but it's still a non-negligible cost on a framework that already has quite a few costs.\n\n## Final notes\n\nBecause I'm sure I'll get comments about it, I'm of course not arguing that you shouldn't encapsulate components if they form the base of your app's design language. If you need to define commmon elements that are \"style-only\" but that still represent a different logical block, say `Card` vs `Paragraph`, then sure go ahead. When you open a render function with `<Paragraph>...</Paragraph>` and `<Card>...</Card>`, you'll still know that nothing funky is going on because those become the common way to write `p` or `div` in your app and that's fine. It doesn't negates the fact that using styled-components everywhere is going to shit on the readability and reusability of your codebase. Styled components are a net negative in nearly all cases.\n\nSo please, stop doing this:\n\n```tsx\nexport const Glorb = styled.div`\n  border: 1px solid salmon;\n`\n```\nand start doing this:\n```tsx\nexport const glorb = css`\n  border: 1px solid salmon;\n`\n```\n\n\n<br/>\n<br/>\n<RandomPlant />\n<br/>\n<br/>","src/content/posts/styled-components-bad.mdx","c11332c2ab5574aa","styled-components-bad.mdx","tailwind-bad",{id:110,data:112,body:117,filePath:118,digest:119,legacyId:120,deferredRender:19},{title:113,description:114,pubDate:115,rating:116},"Why tailwind is bad","romgrk ranting about tailwind","Nov 29 2023",4,"import RandomPlant from '../../components/client/RandomPlant/RandomPlant.tsx'\n\nSo unlike styled-components, there's actually quite a few things that I appreciate about tailwind, so despite the click-baity title up here, this post should be more balanced in its critique than the last one.\n\n## Why it's good\n\nTailwind is an amazing at one thing, and that is at being a *design framework*. You see, when an expert CSS designer starts a project, they will always establish a set of primitives from which to build their design. Those are a spacing unit, a color palette, shades for each color, and a set of component sizes, minimally.\n\nThe problem with this approach has always been that most developers are not expert CSS designers; they're average CSS designers who are tasked to translate mockups into code, when it's not just designing stuff themselves. So lacking this experience, we skip the whole CSS base design system phase and jump to coding. And that's why we end up seeing `color: #e07b7b` (and 10 different other shades) instead of `color: $danger-600`. It's because most devs don't have the experience or time to do CSS from scratch properly.\n\nAnd that's where tailwind comes in. As I said, it's an amazing design framework and being able to ensure that all the juniors are going to produce somewhat consistent styling with low supervision is a huge benefit. Every app should have a design framework, even if it doesn't have tailwind.\n\n\n## Why it's bad\n\nThis below is a basic user card. I wrote most of it myself, in about 20 minutes. Not great, but for someone who isn't a designer, definitely acceptable for an MVP. This looks fine.\n\n<div class=\"xs:max-sm:w-full w-96 xs:max-sm:rounded-xl rounded-2xl bg-white border-zinc-100 dark:bg-zinc-900 border dark:border-zinc-700 mb-8\">\n  <div class=\"flex flex-col gap-2 p-8\">\n    <div class=\"mb-2 text-2xl font-bold tracking-tight text-gray-900 dark:text-white flex flex-row\">\n      <span class=\"mr-2\">Leanne Graham</span>\n      <div class=\"inline-block text-sm tracking-tight bg-zinc-200 border-zinc-300 dark:bg-zinc-800 border dark:border-zinc-600 py-1 px-2 rounded-md mb-0.5\">deactivated</div>\n    </div>\n    <div class=\"flex flex-row flex-wrap\">\n      <div class=\"inline-block rounded-full px-3 py-1 text-sm font-semibold bg-gray-700 text-gray-200 dark:bg-gray-200 dark:text-gray-700 mr-2 mb-2\">#web</div>\n      <div class=\"inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2 mb-2\">#ML</div>\n      <div class=\"inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2 mb-2\">#engineer</div>\n    </div>\n    <div class=\"font-normal leading-6 text-gray-700 dark:text-gray-400 mb-0.5\">I revolutionize end-to-end systems by transitionning cutting-edge web services to aggregate real-time technologies.</div>\n    <label class=\"flex cursor-pointer items-center justify-self-end p-1 gap-2\">\n      <span class=\"text-md text-gray-500\">Receive updates</span>\n      <div class=\"relative grid place-content-center\">\n        <input type=\"checkbox\" class=\"peer h-6 w-12 cursor-pointer appearance-none rounded-full border border-gray-300 bg-white checked:border-gray-900 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-gray-900 focus-visible:ring-offset-2\" />\n        <span class=\"pointer-events-none absolute start-1 top-1 block h-4 w-4 rounded-full bg-gray-400 transition-all duration-200 peer-checked:start-7 peer-checked:bg-gray-900\"></span>\n      </div>\n    </label>\n    <div class=\"row gap-1\">\n      <button class=\"inline-block cursor-pointer rounded-md bg-gray-700 px-4 py-3.5 text-center text-sm font-semibold uppercase text-white transition duration-200 ease-in-out hover:bg-gray-800 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-gray-700 focus-visible:ring-offset-2 active:scale-95\">Like</button>\n      <button class=\"inline-block cursor-pointer rounded-md border border-gray-700 bg-transparent px-4 py-3.5 text-center text-sm font-semibold uppercase text-white transition duration-200 ease-in-out hover:bg-gray-800 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-gray-700 focus-visible:ring-offset-2 active:scale-95\">Mute</button>\n    </div>\n  </div>\n</div>\n\nWhat doesn't look fine is this:\n\n```html\n<div class=\"xs:max-sm:w-full w-96 xs:max-sm:rounded-xl rounded-2xl bg-white border-zinc-100 dark:bg-zinc-900 border dark:border-zinc-700\">\n  <div class=\"flex flex-col gap-2 p-8\">\n    <div class=\"mb-2 text-2xl font-bold tracking-tight text-gray-900 dark:text-white flex flex-row\">\n      <span class=\"mr-2\">Leanne Graham</span>\n      <div class=\"inline-block text-sm tracking-tight bg-zinc-200 border-zinc-300 dark:bg-zinc-800 border dark:border-zinc-600 py-1 px-2 rounded-md mb-0.5\">deactivated</div>\n    </div>\n    <div class=\"flex flex-row flex-wrap\">\n      <div class=\"inline-block rounded-full px-3 py-1 text-sm font-semibold bg-gray-700 text-gray-200 dark:bg-gray-200 dark:text-gray-700 mr-2 mb-2\">#photography</div>\n      <div class=\"inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2 mb-2\">#travel</div>\n      <div class=\"inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-semibold text-gray-700 mr-2 mb-2\">#winter</div>\n    </div>\n    <div class=\"font-normal leading-6 text-gray-700 dark:text-gray-400 mb-0.5\">I revolutionize end-to-end systems by transitionning cutting-edge web services to aggregate real-time technologies.</div>\n    <label class=\"flex cursor-pointer items-center justify-between p-1\">\n      <span>Receive updates</span>\n      <div class=\"relative grid place-content-center\">\n        <input type=\"checkbox\" class=\"peer h-6 w-12 cursor-pointer appearance-none rounded-full border border-gray-300 bg-white checked:border-gray-900 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-gray-900 focus-visible:ring-offset-2\" />\n        <span class=\"pointer-events-none absolute start-1 top-1 block h-4 w-4 rounded-full bg-gray-400 transition-all duration-200 peer-checked:start-7 peer-checked:bg-gray-900\"></span>\n      </div>\n    </label>\n    <div class=\"row gap-1\">\n      <button class=\"inline-block cursor-pointer rounded-md bg-gray-700 px-4 py-3.5 text-center text-sm font-semibold uppercase text-white transition duration-200 ease-in-out hover:bg-gray-800 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-gray-700 focus-visible:ring-offset-2 active:scale-95\">Like</button>\n      <button class=\"inline-block cursor-pointer rounded-md border border-gray-700 bg-transparent px-4 py-3.5 text-center text-sm font-semibold uppercase text-white transition duration-200 ease-in-out hover:bg-gray-800 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-gray-700 focus-visible:ring-offset-2 active:scale-95\">Mute</button>\n    </div>\n  </div>\n</div>\n```\n\nThe tailwind website tries to gaslight us into thinking that it's fine to write your styles like this. That's bullshit. This is a *wall of text*, this is not markup code. It's as if all of a sudden we go back to the dark times where newlines hadn't been invented yet. To illustrate their point, they sure give a few nice examples here and there. But those examples are never complete. If you're building a modern website, you also need responsive design, so all the dimensions classes (`w-full`, `h-16`, etc) are double or tripled with additional `sm:...` and `lg:...` prefixes. And light+dark themes are gaining support, so you also need to double your color classes (`text-gray-700`, `bg-blue-100`, etc) with `dark:...`. The example above is a realistic one, and it can get much worse than that.\n\nI made the mistake of purchasing TailwindUI for a new project some time ago. The initial productivity gain is amazing and it looks great initially, but the moment you start trying to make changes, you start reconsidering your life choices. You see, if I wanted to update the card's \"deactivated\" status above in a BEM project, I'd quickly locate the `card__status` and proceed to update the class. With the monstrosity above, I need to locate the `inline-block text-sm tracking-tight bg-zinc-200 border-zinc-300 dark:bg-zinc-800 border dark:border-zinc-600 py-1 px-2 rounded-md mb-0.5` element. And then when you start updating it, blink for a moment and you might very well loose track of where the element was because it's drowned in a sea of inline styles. And if you need to scroll around to refer to styles a few lines away, you better keep a second window open because there is no way you're finding that element again. Using tailwind this way is basically inline styles and it's *wrong*. You get an amazing DX for the first D who writes the MVP component, and every subsequent D that follows gets the worst DX ever since inline styles.\n\nAnd that's also what you get everytime you open your devtools. Your DOM tree is nothing but a downpour of tailwind classes that overflow the already cramped panel. If I open my DOM tree, I want to see that this div is a `card__title` or a `list__item`. Seeing a `[insert stream of tailwind classes]` does nothing to tell me what kind of element I have in front of my eyes, be it in the devtools or in the source code. DX isn't about what feels best to write fast, DX is about what gives the best experience over the whole period during which the code needs to be read, debugged & updated.\n\nThe other failing of tailwind is that, if used as intended by its creator, the styles aren't shared. If I want to build a \"button\" style, there is no obvious recommended way to build it from say `bg-blue-500 text-white font-semibold active:bg-blue-400` and share it across many locations. So what ends up happening is tons of duplication. Wanna update button-like styles? Go hunt down all those duplicates and update them one by one.\n\nBeing able to give good semantic names is one of the most important aspects of programming. As a wise stackoverflow user once said, *\"the process of naming makes you face the horrible fact that you have no idea what the hell you're doing\"*.\n\n### Summarizing\n\nSo if we recap why tailwind is bad:\n\n - It makes it hard to read the element tree\n - It makes it hard to update styles\n - It doesn't recommend a way to share styles\n\n## How it can be good\n\nSo that being said, I'd like to repeat again (because I'm sure I'll get angry comments on it) that **tailwind is a great CSS framework**. It enforces base style consistency, and that's a good thing. If you write CSS without a framework that fullfills the same role as tailwind, you're doing CSS wrong.\n\nBut it feels like tailwind missed a step. That step, which I came across recently, is [daisyUI](https://daisyui.com/). It turns your code from something like this:\n\n```html\n<button class=\"bg-indigo-600 px-4 py-3 text-center text-sm font-semibold inline-block text-white cursor-pointer uppercase transition duration-200 ease-in-out rounded-md hover:bg-indigo-700 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-indigo-600 focus-visible:ring-offset-2 active:scale-95\">\n  Button\n</button>\n```\n\nInto something like this:\n\n```html\n<button class=\"btn btn-primary\">\n  Button\n</button>\n```\n\nThis here is simple, readable & reusable code.\n\nThe problem with tailwind classes is they are the next step right above inline CSS styles. They're a good design language, but they're not enough to build a design system. If you're building a design system, you want good semantic names. If I add the style `bg-blue-500` on my button, I want to be able to tweak that color later. The actual color name I will want when it's time to update the styles is something like `bg-primary-500` or `bg-info-500`, because it provides a semantic value and can be tweaked separately when the designer suddenly decides that our primary color now *needs* to be flashy pink because all his friends are using it (or maybe because they're all *not* using it).\n\nThe other acceptable way to use tailwind that I've found is to use it with regular CSS, update the tailwind config to get semantic color names, and use `@apply`:\n\n```css\n.btn--primary {\n  @apply text-white;\n  @apply bg-primary-500;\n}\n```\n\n```html\n<button class=\"btn btn--primary\">\n  Button\n</button>\n```\n\nYou still get simple, readable & reusable code.\n\n*\"But what if I need to add a margin to some buttons, won't I need to create tons of modifiers for each and every element?\"*\n\nWell no. You see the goal is always to have readable & reusable code. If the best way to reach that goal is to use an occasional tailwind class here and there, you just use it:\n\n```html\n<button class=\"btn btn--primary mb-1\">\n  Button\n</button>\n```\n\nWe did this in a team I worked with a few months ago, and our non-enforced rule was *\"if you need more than 3-4 tailwind classes, you should probably make it a new class\"*. Worked well, we got the best of both worlds.\n\nSo please, don't ever use tailwind again as tailwind says you should use it.\n\n<br/>\n<br/>\n<RandomPlant />\n<br/>\n<br/>","src/content/posts/tailwind-bad.mdx","a6b3e0e2c9e271e5","tailwind-bad.mdx","test",{id:121,data:123,body:126,filePath:127,digest:128,legacyId:129,deferredRender:19},{title:124,description:28,pubDate:125,draft:19},"Test page","Jan 01 2023","import { DemoTransistor } from '../../components/client/posts/computers-from-scratch/transistor.tsx'\nimport {\n  DemoAndGate,\n  DemoAllGates,\n} from '../../components/client/posts/computers-from-scratch/logicGates.tsx'\nimport {\n  DemoSRNandLatch,\n} from '../../components/client/posts/computers-from-scratch/memory.tsx'\n\ncontent\n\n<DemoSRNandLatch client:load />","src/content/posts/test.mdx","773104eae8aa7fc9","test.mdx","zen-and-software",{id:130,data:132,body:137,filePath:138,digest:139,legacyId:140,deferredRender:19},{title:133,description:134,pubDate:135,sidebar:136},"Zen and the art of software engineering","romgrk on readability","Oct 22 2024",{display:19,depth:105},"import RandomPlant from '../../components/client/RandomPlant/RandomPlant.tsx'\nimport Aside from '../../components/Aside.astro'\n\nI've found over the years that there are two sides to programming. The first side is computer science, and it's about data structures, algorithms and CPUs. The second side, I call software engineering. This side is about creating readable and maintainable codebases that can evolve over time, and just like Quality in *Zen and the art of motorcycle maintenance*, is hard to quantify, but easy to qualify: you know it when you see it. This post is a counterpart to [Optimizing JS for fun and for profit](./optimizing-javascript), where instead of focusing on the performance part of code, I explain the various practices that have helped me create readable and maintainable code.\n\n### 1. Constraints\n\nMy last post on [Efficient Typescript](./efficient-typescript) touched a bit on the subject, but I cannot emphasize enough how much establishing constraints is an essential part of maintainable code. Constraints give you the possibility of making assumptions later on that will simplify your codebase.\n\nA common way to spot if you are not strict enough is that you often end up adding a lot of defensive checks, for example:\n\n```javascript\nfunction findUser(id) {\n  // avoid SQL injection\n  if (typeof id !== 'number')\n    throw new Error('ID is not set')\n\n  return db.select(`SELECT * FROM users WHERE id = ${id}`)\n}\n```\n\nWhat this block of code should really look like is this:\n\n```typescript\nfunction findUser(id: number) {\n  return db.select(`SELECT * FROM users WHERE id = ${id}`)\n}\n```\n\nNow yes, typescript can only help you so much, and you'll sometimes need to add runtime validation checks. That's ok. But **there's only ever two places where there should be runtime validation: user input and network data**. Eveything else is part of a machine that is fully in your control. The machine can accept fuzzy invalid user-input (and fail appropriately), but it should not contain any fuzziness in its insides. No loose screw, no duct tape. Validate your inputs, and then establish ruthlessly strict constraints for how the inside of the machine works.\n\n<Aside title=\"Low-level aside\">\n  Even stronger constraints can also enable even greater power. As an anecdote, the other day I was writing a PR for a Rust codebase I wasn't familiar with that had multi-threaded code. As I was coding, I felt like it would have been possible to switch from a (multi-thread safe but expensive) `Arc<Mutex<T>>` to a (single-threaded only) `Rc<RefCell<T>>`. It sounds like magic but all I had to do to validate that it was safe, was to do the change and let the compiler compile. It compiled, thus it was safe. Doing the same change in C/C++ would have required hours of careful reading of the full system before being satisfied that the change was safe. The constraints placed by the Rust language enabled me to write correct code easily.\n</Aside>\n\nBut those contraints can also be external. Early internet pioneer Jon Postel said *\"be conservative in what you send, be liberal in what you accept\"*. I'm sorry Jon but I don't buy this one. By being liberal in what you accept, wouldn't you be encouraging others to build unsound systems? By building a \"robust\" system that accepts unstrict input, aren't you encouraging the ecosystem that depends on you to not be robust? In the name of immediate gains, aren't you condemning the future internet to pay the cost of maintaining the early flaws forever? I say **be strict in what you accept; make your API options limited to the essential; find the best way to do a thing, and don't expose a second way to do it**. Yes I'm sure you have an exception to that. Maybe you're right, maybe you're not. There are no quantitative rules here, only qualitative judgements.\n\n#### 1.1 Empty value\n\nAnother common way in which the lack of constraints manifests itself is frequent null/undefined checks. There is a concept in programming called the **empty value**, and remembering these two questions helps improve constraints:\n\n - What is a good empty value for this type?\n - Do I need an empty value?\n\nA few examples of empty value:\n - The empty string (`''`) for `string` type\n - The number `-1` for array index positions\n - `null` or `undefined` in conjunction with any type\n - `NaN` for float values\n - An empty array (`[]`) for any array type\n\nI see too often people defaulting to `null`/`undefined` as a way to signal that the value hasn't been set yet. But do you really need an empty value? An example.\n\n```typescript\ntype Dimension = { width: number, height: number }\n\n// This is our component's dimensions. It's\n// calculated on the first render, so we don't have\n// a value for it yet. Let it be null for now.\nlet rootDimensions = null as Dimension | null\n```\n\nIt might not be immediately apparent, but the consequence of picking `null` as the empty value is that it condemns every single subsequent section of code that uses `rootDimensions` to add null checks and default values:\n\n```typescript\nlet sidebarWidth = (rootDimensions?.width ?? 0) * 0.20\n//                                ^^^^^^^^^^^^\n// This is going to be repeated *each* time `rootDimensions` is used.\n```\n\nWhereas initializing the dimensions with a monomorphic value negates the need for later null checks:\n\n```typescript\nconst EMPTY_DIMENSIONS = { width: 0, height: 0 } as Dimension \n\nlet rootDimensions = EMPTY_DIMENSIONS // no more `| null` needed!\n```\n\n<Aside title=\"A note on performance\">\n  For those who spotted that \"monomorphic\" also ties into my post on performance, yes, having a monomorphic value not only helps readability but also greatly improves performance: [Optimizing JS: Avoid different shapes](./optimizing-javascript#2-avoid-different-shapes).\n</Aside>\n\n#### 1.2 Conclusion\n\nSo the question I always ask myself is, **which constraints can I place on my system to ensure the least effort needed down the line?** Which nullable values can you get rid of? Which APIs can you simplify?\n\n<RandomPlant className='mt-8 mb-16' />\n\n\n### 2. Semantics\n\nCode is the halfway point between how a machine needs data to be formatted to be executed and how a human needs data to be formatted to be understood. Writing code is both being able to make it correct and performant, and writing it in a way that conveys the proper semantics to the next person who will read it.\n\nFor a machine, a value is a number.  \nFor a human, a value has a meaning.  \nFor a programmer, a value is the union of both.\n\nUse a value not only because it's the number you need, but also because it has the right meaning. A perfect example of the application of this principle is being able to re-assign a variable to a new variable without changing its value, simply because we need a new meaning for it. To take an example from a codebase I've worked on lately:\n\n```typescript\nconst isVirtualRow = rowIndexInPage === virtualRowIndex\nconst isNotVisible = isVirtualRow\n```\n\nIntroducing new names and concepts is as important as introducing new values. You need to code for the right balance of computer/human understanding. If you go too far, it becomes harder for the other side to process it.\n\nIf you find yourself struggling to find a name for a new concept, take time to pause and figure one out. The name you pick for that concept—be it a variable, a function, or the main class your whole application will depend on—will have an influence on how you and future programmers understand and solve the problem domain. If you don't have a good satisfying name, give it your best try but more importantly **come back to refactor it as soon as you have understood the concept better**.\n\nMy all-time favourite programming quote summarizes well how important naming is:\n\n> Take time to find good names and take time to re-factor those names as much as necessary. As a wise stackoverflow user once said, the process of naming makes you face the horrible fact that you have no idea what the hell you're doing.  \n>\n> ― A reddit user\n\nSo which names can you refactor?\n\n#### Declarative programming\n\nWriting code in a declarative way is also very helpful for readability, and functional programming in particular is very suitable for this. Take for example these two versions:\n\n<div class='code-blocks'>\n\n```javascript\nlet result = 0\nfor (let i = 0; i < numbers.length; i++) {\n  let n = Math.round(numbers[i] * 10)\n  if (n % 2 !== 0) continue\n  result = result + n\n}\n```\n\n```javascript\nconst result =\n  numbers\n    .map(n => Math.round(n * 10))\n    .filter(n => n % 2 === 0)\n    .reduce((a, n) => a + n, 0)\n```\n\n</div>\n\nThe second one tells you right away the operations that it's going to do: a transform (`map`), a filtering pass, and then a reduction to a final value. To get all that information, you only have to pick up the `map`, `filter`, and `reduce` symbols. Whereas for the first version, you need to read the full code to pick up that information.\n\nUsing FP and in general any shared programming vocabulary (e.g. design patterns) is a good way to communicate what the code is doing in a short and elegant way. By default, write code with the conventions that are already in use by other programmers.\n\nAnd yes, that example is the inverse of [Optimizing JS: Avoid array/object methods](./optimizing-javascript#3-avoid-arrayobject-methods). One version isn't better than the other. Often, programming is about picking which of performance or maintainability is more important.\n\n\n<RandomPlant className='mt-8 mb-16' />\n\n### 3. Beauty\n\nSome would have you think that software engineering is a cold discipline, nothing could be further from the truth. Writing software is an art, it's composing a virtual machine with logic constructs made visible with written symbols, symbols that communicate with the next human who will read them. Which of us hasn't known the joy of completing a function, a module, with the knowledge that it runs with the utmost elegance, efficiency and simplicity that we could ever come up with? Material-world artisans can build wonderful contraptions made of lights, rolling balls, levers and pullies. So can we, but ours are made of logic constructs, invisible to the untrained eye.\n\nThe way we write softare has a huge influence on how it is perceived by others. The brain is a pattern-matching system that relies on recurring patterns to ease its processing. Inserting symmetry, spacing and alignment—in other words, beauty—in our code makes it easier to read. For example, I initially wrote these words you saw sooner in this format:\n\n> For a machine, a value is a number. For a human, a value has a meaning. For a programmer, a value is the union of both.\n\nAlthough the conventional way to write paragraphs is the one above, I know that inserting newlines before the \"For\" would break the text in easier to consume bits, as the brain can see the recurring \"For\", the somewhat aligned \"a value...\", and run its circuits in harmony:\n\n> For a machine, a value is a number.  \n> For a human, a value has a meaning.  \n> For a programmer, a value is the union of both.\n\nWriting code isn't just about semantics, it's also about writing it in a way that is pleasant to the eye. Here are a few patterns that I frequently follow.\n\n#### Building pyramids\n\nI always sort code lines this way, and I call these pyramids. Tell me, which one of these blocks seems easier to read for you?\n\n<div class='code-blocks'>\n\n```javascript\n{\n  hasUninitialized: items.some(i => i.status === Status.Uninitialized),\n  hasReady: items.some(i => i.status === Status.Ready),\n  hasInactive: items.some(i => i.status === Status.Inactive),\n  hasPending: items.some(i => i.status === Status.Pending),\n  hasCompleted: items.some(i => i.status === Status.Completed),\n}\n```\n\n```javascript\n{\n  hasReady: items.some(i => i.status === Status.Ready),\n  hasPending: items.some(i => i.status === Status.Pending),\n  hasInactive: items.some(i => i.status === Status.Inactive),\n  hasCompleted: items.some(i => i.status === Status.Completed),\n  hasUninitialized: items.some(i => i.status === Status.Uninitialized),\n}\n```\n\n</div>\n\nWhen I read the second, the eye can flow from one line to the next, harmoniously.\n\n#### Aligning names & symbols\n\nI also find it particularly important to align sub-sections of identifiers (e.g. common prefixes), as well as common symbols like `=` assignements.\n\n<div class='code-blocks'>\n\n```typescript\nconst DEFAULT_ROW_GROUPING_STRATEGY = 'default'\nconst DATA_SOURCE_ROW_GROUPING_STRATEGY = 'data-source'\n```\n\n```typescript\nconst ROW_GROUPING_STRATEGY_DEFAULT     = 'default'\nconst ROW_GROUPING_STRATEGY_DATA_SOURCE = 'data-source'\n```\n\n</div>\n\nAlthough the names sound more natural when pronounced out loud for the first block, they're much easier to read as a whole in the second one.\n\n#### Aligning for logic\n\nAnother way in which alignment can be helpful is when we need have operations on mostly similar expression, with slightly differing semantics. The example below is from a python codebase I worked on, where we split a dataframe into two sub-sections. In this particular case I had to fight the auto-formatter (and colleagues) to have it formatted properly.\n\n<div class='code-blocks'>\n\n```javascript\n// code as the auto-formatter wanted it\naverage = [df > stats.average, df <= stats.average]\nq1 = [df > stats.q1, df <= stats.q1]\noutliers = [df > stats.outliers, df <= stats.outliers]\neven = [df % 2 == 0, df % 2 == 1]\n```\n\n```javascript\n// code as I wanted it\naverage = [\n  df >  stats.average,\n  df <= stats.average,\n]\nq1 = [\n  df >  stats.q1,\n  df <= stats.q1,\n]\noutlier = [\n  df >  stats.outlier,\n  df <= stats.outlier,\n]\neven = [\n  df % 2 == 0,\n  df % 2 == 1,\n]\n```\n\n</div>\n\nWhich of those block would you prefer to code review? The auto-formatted one, or the manually formatted one? For all their benefits, **full-code formatters like `prettier` have gotten many programmers convinced that even talking about formatting and alignment is a negative**, best left to machines to deal with. Code is not just about machines. It's about humans.\n\nI don't want to have to argue if the `{` should be after the `if` or on a line of its own. `eslint` can do that well enough, no need to let `prettier` destroy beauty. Here is an example from some color code, in typescript:\n\n<div class='code-blocks'>\n\n```typescript\n// code as prettier wants it\nexport function newColor(r: number, g: number, b: number, a: number) {\n  return (r << OFFSET_R) + (g << OFFSET_G) + (b << OFFSET_B) + (a << OFFSET_A);\n}\n```\n\n```typescript\n// code as it should be\nexport function newColor(r: number, g: number, b: number, a: number) {\n  return (\n    (r << OFFSET_R) +\n    (g << OFFSET_G) +\n    (b << OFFSET_B) +\n    (a << OFFSET_A)\n  );\n}\n```\n\n</div>\n\n#### Spacing for clarity\n\nAnd if the last examples I went for using more vertical space, the opposite can also improve readability.\n\n<div class='code-blocks'>\n\n```typescript\nswitch (priority) {\n  case DiscreteEventPriority:\n    priority = ImmediatePriority;\n    break;\n  case ContinuousEventPriority:\n    priority = UserBlockingPriority;\n    break;\n  case DefaultEventPriority:\n    priority = NormalPriority;\n    break;\n  case IdleEventPriority:\n    priority = IdlePriority;\n    break;\n  default:\n    priority = NormalPriority;\n    break;\n}\n```\n```typescript\nswitch (priority) {\n  case DiscreteEventPriority:   priority = ImmediatePriority;    break;\n  case ContinuousEventPriority: priority = UserBlockingPriority; break;\n  case DefaultEventPriority:    priority = NormalPriority;       break;\n  case IdleEventPriority:       priority = IdlePriority;         break;\n  default:                      priority = NormalPriority;       break;\n}\n```\n</div>\n\n The amount of spacing to use for code follows the same rule that designers use for adding spacing in their designs: **use your eyes**. They will tell you if you need more or less. They will tell you if what they're seeing is beautiful or not.\n\n\n<RandomPlant className='mt-8 mb-16' />\n\n### 4. Simplicity\n\nI think if I had to summarize software engineering in one expression, it would be **managing complexity**. Most of our work as software engineers is keeping complexity in check with all the tools we have. One of the characteristics that code should have above all is *simplicity*. Simple code is easy to read, easy to modify and easy to debug. Code that isn't simple is much harder to also make *correct* and *performant*, thus we should always aim for simplicity first.\n\nIn fact, if features are an asset, code is a liability. A general rule, and I think everyone will agree with me, is that the more code you have, the more bugs you have. So a software product in theory should aim to have as many features as possible, while having as little code as possible. That's why one of my favourite activities as a programmer is deleting large sections of code.\n\nIn practice, in the front-end world this manifest itself as implementing features using browser APIs and CSS instead of re-building existing features in javascript (please no more horror like CSS-in-JS ever again).\n\n<RandomPlant className='mt-8 mb-16' />\n\n### 5. Pleasure\n\nAs a last point, I think I need to mention that the root of all qualities in software engineering is *pleasure*. You write good, elegant, beautiful, simple code because you find pleasure in writing software. If you don't find pleasure in it, you will never put the effort to make any of the above qualities emerge, and you won't find joy in your craft.\n\n<RandomPlant className='mt-8 mb-16' />\n\n### Conclusion\n\nI hope I was able to pass along some of the zen of programming to you. I can't claim credit for any of the points here as they've all been transmitted to me throughout years of programming. I unfortunately can't remember where I picked up each of those ideas, but I think I owe at least some of them to [Uncle Bob](https://blog.cleancoder.com/), [Joel on software](https://www.joelonsoftware.com/) and so many others.\n\n<RandomPlant className='mt-8 mb-16' />","src/content/posts/zen-and-software.mdx","260f3ec500c9d0d9","zen-and-software.mdx"];

export { _astro_dataLayerContent as default };
